{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6ab8945",
   "metadata": {},
   "source": [
    "# Assignment 3 - Deep Learning\n",
    "# Names: Roy Madpis, Michael Kobiavanov\n",
    "## IDs: 319091526, 206814485\n",
    "## Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73115c84",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-08T19:19:26.857403Z",
     "start_time": "2022-01-08T19:19:14.620884Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: prettytable in c:\\users\\roy\\anaconda3\\lib\\site-packages (2.4.0)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\roy\\anaconda3\\lib\\site-packages (from prettytable) (0.2.5)\n"
     ]
    }
   ],
   "source": [
    "! pip install prettytable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1455e182",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T13:45:07.006534Z",
     "start_time": "2022-01-12T13:44:56.213603Z"
    }
   },
   "outputs": [],
   "source": [
    "#import imageio\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from PIL import Image \n",
    "\n",
    "import time\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import MinMaxScaler \n",
    "from sklearn.preprocessing import StandardScaler \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#################################################################\n",
    "#The following function compute the number of parameters in each layer in a neural net model\n",
    "#and stores the total number of parameters\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "def count_parameters(model):\n",
    "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad: continue\n",
    "        param = parameter.numel()\n",
    "        table.add_row([name, param])\n",
    "        total_params+=param\n",
    "    print(table)\n",
    "    print(f\"Total Trainable Params: {total_params}\")\n",
    "    return total_params\n",
    "    \n",
    "#total_params = count_parameters(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6156f87",
   "metadata": {},
   "source": [
    "# <font color = crimson > Question 1 (70%) : </font>\n",
    "\n",
    "CIFAR-10 is another MNIST-like repository of images.\n",
    "\n",
    "### a. Your task is to explore performance of several neural network architectures.\n",
    "**Choose combinations of:**\n",
    "+ fully connected\n",
    "+ convolutional\n",
    "+ dropout\n",
    "+ max pooling\n",
    "+ and batch normalization layers.\n",
    "\n",
    "**Then :**\n",
    "- Compare rates of training and inference (time per sample)\n",
    "- and accuracy on **train**, **validation**, and **test sets**.\n",
    "- Present your results in informative, simple to understand and visually appealing manner (e.g. summary table or graphs). \n",
    "- Explain your conclusions (briefly). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c722abe",
   "metadata": {},
   "source": [
    "## Step 1: CIFAR-10 Data Loading + change to tensor type \n",
    "+ Each row of the array stores a 32x32 colour image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15e5d2f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-08T19:19:36.357409Z",
     "start_time": "2022-01-08T19:19:36.344418Z"
    }
   },
   "outputs": [],
   "source": [
    "# from Mnist_Data.data_utils import load_CIFAR10\n",
    "# cifar10_dir = 'Mnist_Data/cifar-10-batches-py'\n",
    "\n",
    "# # Cleaning up variables to prevent loading data multiple times (which may cause memory issue)\n",
    "# try:\n",
    "#     del X_train, y_train\n",
    "#     del X_test, y_test\n",
    "#     print('Clear previously loaded data.')\n",
    "# except:\n",
    "#     pass\n",
    "\n",
    "# X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "# #reshaping to get data in shape: KX3X32X32 #k=num of images\n",
    "# X_train = X_train.reshape(-1,3,32,32)\n",
    "# X_test = X_test.reshape(-1,3,32,32)\n",
    "\n",
    "# #changing to type Tensor\n",
    "# X_train_t = torch.tensor(X_train, dtype = torch.uint8)\n",
    "# X_test_t = torch.tensor(X_test, dtype = torch.uint8)\n",
    "# y_train_t = torch.tensor(y_train, dtype = torch.uint8)\n",
    "# y_test_t = torch.tensor(y_test, dtype = torch.uint8)\n",
    "\n",
    "# #creating validation set -  splitting to training and validation set\n",
    "# X_train_t, X_validation_t, y_train_t, y_validation_t=train_test_split(X_train_t, y_train_t, test_size = 0.05,\n",
    "#                                                                       shuffle=True, random_state = 1998)\n",
    "\n",
    "# # As a sanity check, we print out the size of the training and test data.\n",
    "\n",
    "# print('Training data shape: ', X_train_t.shape) #print('Training data shape: ', X_train.shape)\n",
    "# print('Training labels shape: ', y_train_t.shape) #print('Training labels shape: ', y_train.shape)\n",
    "# print(\"---------------------------------------------------------\")\n",
    "# print('Validation data shape: ', X_validation_t.shape)\n",
    "# print('Validation labels shape: ', y_validation_t.shape)\n",
    "# print(\"---------------------------------------------------------\")\n",
    "# print('Test data shape: ', X_test_t.shape) #print('Test data shape: ', X_test.shape)\n",
    "# print('Test labels shape: ', y_test_t.shape) #print('Test labels shape: ', y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10b01795",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T13:45:11.778852Z",
     "start_time": "2022-01-12T13:45:07.038740Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "cifar_train = torchvision.datasets.CIFAR10(root=\"./data\", train = True, download = True)\n",
    "cifar_test = torchvision.datasets.CIFAR10(root=\"./data\", train = False, download = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06f51c27",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T13:45:19.115993Z",
     "start_time": "2022-01-12T13:45:11.910646Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading the data, this may take some time!\n",
      "\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Training data shape:  torch.Size([47500, 3, 32, 32])\n",
      "Training labels shape:  torch.Size([47500])\n",
      "---------------------------------------------------------\n",
      "Validation data shape:  torch.Size([2500, 3, 32, 32])\n",
      "Validation labels shape:  torch.Size([2500])\n",
      "---------------------------------------------------------\n",
      "Test data shape:  torch.Size([10000, 3, 32, 32])\n",
      "Test labels shape:  torch.Size([10000])\n"
     ]
    }
   ],
   "source": [
    "# load data - this may take some time!\n",
    "print(\"loading the data, this may take some time!\")\n",
    "print()\n",
    "cifar_train = torchvision.datasets.CIFAR10(root=\"./data\", train = True, download = True)\n",
    "cifar_test = torchvision.datasets.CIFAR10(root=\"./data\", train = False, download = True)\n",
    "#################################################\n",
    "x_train = cifar_train.data\n",
    "y_train = cifar_train.targets\n",
    "x_test = cifar_test.data\n",
    "y_test = cifar_test.targets\n",
    "\n",
    "#reshaping to get data in shape: KX3X32X32 #k=num of images\n",
    "X_train = x_train.reshape(-1,3,32,32)\n",
    "X_test = x_test.reshape(-1,3,32,32)\n",
    "#changing to type Tensor\n",
    "X_train_t = torch.tensor(X_train, dtype = torch.uint8)\n",
    "X_test_t = torch.tensor(X_test, dtype = torch.uint8)\n",
    "y_train_t = torch.tensor(y_train, dtype = torch.uint8)\n",
    "y_test_t = torch.tensor(y_test, dtype = torch.uint8)\n",
    "\n",
    "#creating validation set -  splitting to training and validation set\n",
    "X_train_t, X_validation_t, y_train_t, y_validation_t=train_test_split(X_train_t, y_train_t, test_size = 0.05,\n",
    "                                                                      shuffle=True, random_state = 1998)\n",
    "\n",
    "# As a sanity check, we print out the size of the training and test data.\n",
    "\n",
    "print('Training data shape: ', X_train_t.shape) #print('Training data shape: ', X_train.shape)\n",
    "print('Training labels shape: ', y_train_t.shape) #print('Training labels shape: ', y_train.shape)\n",
    "print(\"---------------------------------------------------------\")\n",
    "print('Validation data shape: ', X_validation_t.shape)\n",
    "print('Validation labels shape: ', y_validation_t.shape)\n",
    "print(\"---------------------------------------------------------\")\n",
    "print('Test data shape: ', X_test_t.shape) #print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test_t.shape) #print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7960d8d",
   "metadata": {},
   "source": [
    "## Step 2 - Normalizing the data\n",
    "\n",
    "As we are dealing with **images**, then we know that each pixel can have a value between 0 to 255, thus if we divide the X vector with 255 we will get values between 0 to 1.\n",
    "\n",
    "**Note** that before normalizing the data, we must convert it first into **torch.float32**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a29978a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T13:45:33.324585Z",
     "start_time": "2022-01-12T13:45:22.690353Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max value after normalization:  1.0\n",
      "Min value after normalization:  0.0\n"
     ]
    }
   ],
   "source": [
    "X_train_t_normalize = X_train_t.to(dtype = torch.float32)/255\n",
    "X_validation_t_normalize = X_validation_t.to(dtype = torch.float32)/255\n",
    "X_test_t_normalize = X_test_t.to(dtype = torch.float32)/255\n",
    "\n",
    "#sanity check: we need to get that the max value is 1 and that the min value is 0:\n",
    "print(\"Max value after normalization: \", torch.max(X_train_t_normalize).item()) # the .item() will retrieve only the value\n",
    "print(\"Min value after normalization: \", torch.min(X_validation_t_normalize).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be9fa857",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T13:45:34.266926Z",
     "start_time": "2022-01-12T13:45:34.051953Z"
    }
   },
   "outputs": [],
   "source": [
    "#The function retrives 4 outputs regarding the shape we will get after the conv layer:\n",
    "def conv_output_shape(conv_layer, H_in = 32, W_in = 32, verbose = True):\n",
    "    out_channels = conv_layer.out_channels\n",
    "    in_channels = conv_layer.in_channels\n",
    "    kernel_size = conv_layer.kernel_size\n",
    "    stride = conv_layer.stride\n",
    "    padding = conv_layer.padding\n",
    "    dilation = conv_layer.dilation\n",
    "    \n",
    "    H_out = math.floor(((H_in + 2*padding[0] - dilation[0]*(kernel_size[0] - 1) -1)/stride[0]) +1)\n",
    "    W_out =  math.floor(((W_in + 2*padding[1] - dilation[1]*(kernel_size[1] - 1) -1)/stride[1]) +1)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Input Channels =\", in_channels, \" | Output channels =\", out_channels)\n",
    "        print(\"Output shape: num_images X\", out_channels, \"X\", H_out, \"X\", W_out)\n",
    "    return out_channels, H_out, W_out\n",
    "\n",
    "\n",
    "\n",
    "def conv_maxpool_output_shape(conv_layer, maxpoolayer, H_in = 32, W_in = 32, verbose = True):\n",
    "    out_channels = conv_layer.out_channels\n",
    "    in_channels = conv_layer.in_channels\n",
    "    kernel_size = conv_layer.kernel_size\n",
    "    stride = conv_layer.stride\n",
    "    padding = conv_layer.padding\n",
    "    dilation = conv_layer.dilation\n",
    "    H_out_conv = math.floor(((H_in + 2*padding[0] - dilation[0]*(kernel_size[0] - 1) -1)/stride[0]) +1)\n",
    "    W_out_conv =  math.floor(((W_in + 2*padding[1] - dilation[1]*(kernel_size[1] - 1) -1)/stride[1]) +1)\n",
    "    \n",
    "    kernel_size_pool = maxpoolayer.kernel_size\n",
    "    stride_pool = maxpoolayer.stride\n",
    "    padding_pool = maxpoolayer.padding\n",
    "    dilation_pool = maxpoolayer.dilation\n",
    "    #H_out_pool = math.floor(((H_out_conv + 2*padding_pool[0] - dilation_pool[0]*(kernel_size_pool[0] - 1) -1)/stride_pool[0]) +1)\n",
    "    #W_out_pool =  math.floor(((W_out_conv + 2*padding_pool[1] - dilation_pool[1]*(kernel_size_pool[1] - 1) -1)/stride_pool[1]) +1)\n",
    "    H_out_pool = math.floor(((H_out_conv + 2*padding_pool - dilation_pool*(kernel_size_pool - 1) -1)/stride_pool) +1)\n",
    "    W_out_pool =  math.floor(((W_out_conv + 2*padding_pool - dilation_pool*(kernel_size_pool - 1) -1)/stride_pool) +1)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Input Channels =\", in_channels, \" | Output channels =\", out_channels)\n",
    "        print(\"Output shape: num_images X\", out_channels, \"X\", H_out_pool, \"X\", W_out_pool)\n",
    "        \n",
    "    return out_channels, H_out_pool, W_out_pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d20c92a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T13:45:34.763859Z",
     "start_time": "2022-01-12T13:45:34.653533Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: 1 X 3 X 32 X 32\n"
     ]
    }
   ],
   "source": [
    "num_images = 1\n",
    "C_in = 3\n",
    "H_in = 32\n",
    "W_in = 32\n",
    "#input_layer = [C_in*H_in*W_in]\n",
    "print(\"Input shape:\", num_images, \"X\", C_in, \"X\", H_in, \"X\", W_in)\n",
    "#######################################3\n",
    "out_channels = 50\n",
    "kernel_size = (5,5) #Size of the convolving kernel\n",
    "stride = (7,5) #stride (int or tuple, optional) â€“ Stride of the convolution. Default: 1\n",
    "\n",
    "padding = (4, 4) #Padding added to all four sides of the input. Default: 0\n",
    "dilation = (1,1) # Spacing between kernel elements. Default: 1 - default = (1, 1)\n",
    "conn1 = torch.nn.Conv2d(in_channels = C_in,\n",
    "                out_channels = out_channels,\n",
    "                kernel_size = kernel_size,\n",
    "                stride = stride,\n",
    "                padding = padding,\n",
    "                dilation = dilation) \n",
    "\n",
    "\n",
    "maxpoolayer = torch.nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "222828d9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T13:45:35.785103Z",
     "start_time": "2022-01-12T13:45:35.163179Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input was one image, the input shape: torch.Size([100, 3, 32, 32])\n",
      "Conv Output shape: torch.Size([100, 50, 6, 8])\n",
      "Conv Output shape: torch.Size([100, 50, 3, 4])\n",
      "-----------------------------------------------------------------------\n",
      "Input Channels = 3  | Output channels = 50\n",
      "Output shape: num_images X 50 X 3 X 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(50, 3, 4)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_image = X_train_t_normalize[0:100]\n",
    "\n",
    "print(\"Input was one image, the input shape:\", one_image.shape)\n",
    "conv_output = conn1(one_image)\n",
    "maxpool_output = maxpoolayer(conv_output)\n",
    "print(\"Conv Output shape:\", conv_output.shape)\n",
    "print(\"Conv Output shape:\", maxpool_output.shape)\n",
    "\n",
    "print(\"-----------------------------------------------------------------------\")\n",
    "conv_maxpool_output_shape(conv_layer=conn1, maxpoolayer=maxpoolayer, H_in = 32, W_in = 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc238ae",
   "metadata": {},
   "source": [
    "## Defining the 1st model -   <font color = purple > Net models class: </font> \n",
    "\n",
    "### This class enables defining a neural net model.\n",
    "\n",
    "The class needs to get as input:\n",
    "1. **input_layer** = the dimension of the input layer (insert as a list)\n",
    "2. **layers** = a list containing the dimensions  of the different layers ( without The input and output layers)\n",
    "3. **output_layer** =  the dimension of the output layer (insert as a list)\n",
    "\n",
    "\n",
    "4. **drop_out** = if you wish to include drop out in each hidden layer, then pass a float between 0 to 1. **default** = *False*\n",
    "5. **batch_norm** = if you wish to add a batch normalization layer. **default** = *False*\n",
    "\n",
    "\n",
    "Note: the input data has 3 dimensions - R G B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee82f989",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T13:45:36.392384Z",
     "start_time": "2022-01-12T13:45:36.381645Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3072, 512, 256]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_layer = [3*32*32]\n",
    "layers=[512, 256]\n",
    "[input_layer[0]]+ layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "235c5c7c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T13:45:36.971350Z",
     "start_time": "2022-01-12T13:45:36.952801Z"
    }
   },
   "outputs": [],
   "source": [
    "class Net_models(torch.nn.Module): # derive your model from the basic torch.nn.Model\n",
    "    def __init__(self, input_layer = [3*32*32], layers=[512, 256], output_layer = [10], drop_out=False,\n",
    "                batch_norm = False):\n",
    "        super(Net_models,self).__init__()\n",
    "        self.flatten = torch.nn.Flatten()\n",
    "        self.input_layer = input_layer[0]\n",
    "        #output_layer = [10]\n",
    "        layers = input_layer + layers + output_layer # default: 784x512x256x10        \n",
    "        \n",
    "        layers_list = []\n",
    "        for Layer_i in range(1, len(layers)): #1,2,3,... according to the number of layers given\n",
    "            layers_list.append(torch.nn.Linear(layers[Layer_i-1], layers[Layer_i]) )\n",
    "            #self.fully_connected_1 = torch.nn.Linear(28*28, 512)\n",
    "\n",
    "            if Layer_i < len(layers)-1: #if the layer i is not the last layer - add a ReLU \n",
    "                layers_list.append( torch.nn.ReLU() )\n",
    "                if drop_out:\n",
    "                    layers_list.append(torch.nn.Dropout(drop_out))\n",
    "                if batch_norm:\n",
    "                    layers_list.append(torch.nn.BatchNorm1d(layers[Layer_i]))\n",
    "\n",
    "        #if the layer i is the last layer - add a sog-softmax \n",
    "        layers_list.append( torch.nn.LogSoftmax())\n",
    "        \n",
    "        self.layers_stack = torch.nn.Sequential(*layers_list) #this will enable getting params that can be trained\n",
    "        self.layers = layers_list \n",
    "        \n",
    "    def forward(self,x):        \n",
    "        x = x.view(-1,self.input_layer) # flatten image input. This code will work for input as vector or image. will also work for batches. \n",
    "        #x = self.flatten(x)\n",
    "        logits = self.layers_stack(x)\n",
    "        \n",
    "        #for layer_index, layer in enumerate(self.layers): \n",
    "        #    x = self.layer(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d67d71",
   "metadata": {},
   "source": [
    "## Defining the 2nd  model -   <font color = purple > Net models class with covolusion: </font> \n",
    "\n",
    "### This class enables defining a neural net model.\n",
    "\n",
    "The class needs to get as input:\n",
    "1. **input_layer** = the dimension of the input layer (insert as a list)\n",
    "2. **layers** = a list containing the dimensions  of the different layers ( without The input and output layers)\n",
    "3. **output_layer** =  the dimension of the output layer (insert as a list)\n",
    "\n",
    "\n",
    "4. **drop_out** = if you wish to include drop out in each hidden layer, then pass a float between 0 to 1. **default** = *False*\n",
    "5. **batch_norm** = if you wish to add a batch normalization layer. **default** = *False*\n",
    "\n",
    "\n",
    "Note: the input data has 3 dimensions - R G B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2010516d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T13:45:37.575702Z",
     "start_time": "2022-01-12T13:45:37.542506Z"
    }
   },
   "outputs": [],
   "source": [
    "class Net_models_conv(torch.nn.Module): # derive your model from the basic torch.nn.Model\n",
    "    def __init__(self, C_in = 3, H_in = 32, W_in = 32, layers=[512, 256], output_layer = [10],\n",
    "                 drop_out=False,\n",
    "                 batch_norm = False):\n",
    "        super(Net_models_conv,self).__init__()\n",
    "        self.C_in = C_in\n",
    "        self.H_in = H_in\n",
    "        self.W_in = W_in\n",
    "        \n",
    "        ###################################################\n",
    "        conv1 = torch.nn.Conv2d(in_channels = C_in, #3\n",
    "                                     out_channels=20,\n",
    "                                     kernel_size=4,\n",
    "                                     padding = 1) \n",
    "        relu1 = torch.nn.ReLU() \n",
    "        \n",
    "        out_channels_conv1, H_out_conv1, W_out_conv1 = conv_output_shape(conv_layer = conv1,\n",
    "                                                                         H_in = self.H_in,\n",
    "                                                                         W_in = self.W_in, verbose = False)\n",
    "        \n",
    "        ###################################################\n",
    "        conv2 = torch.nn.Conv2d(in_channels= conv1.out_channels, #16\n",
    "                                     out_channels=4,\n",
    "                                     kernel_size=5,\n",
    "                                     padding  = 2) \n",
    "        relu2 = torch.nn.ReLU()\n",
    "\n",
    "        out_channels_conv2, H_out_conv2, W_out_conv2 = conv_output_shape(conv_layer = conv2,\n",
    "                                                                         H_in = H_out_conv1,\n",
    "                                                                         W_in = W_out_conv1, verbose = False)\n",
    "        ###################################################\n",
    "        conv_layers_list = [conv1, relu1, conv2, relu2 ]\n",
    "        self.conv_layers = conv_layers_list\n",
    "        self.conv_layers_stack = torch.nn.Sequential(*conv_layers_list) #this will enable getting params that can be trained\n",
    "\n",
    "        input_layer_for_fully_connected_layer_1 = [out_channels_conv2*H_out_conv2*W_out_conv2]\n",
    "        layers = input_layer_for_fully_connected_layer_1 + layers + output_layer # default: 3072x512x256x10        \n",
    "        \n",
    "        layers_list = []\n",
    "        for Layer_i in range(1, len(layers)): #1,2,3,... according to the number of layers given\n",
    "            \n",
    "            layers_list.append(torch.nn.Linear(layers[Layer_i-1], layers[Layer_i]) )\n",
    "\n",
    "            if Layer_i < len(layers)-1: #if the layer i is not the last layer - add a ReLU \n",
    "                layers_list.append( torch.nn.ReLU() )\n",
    "                if drop_out:\n",
    "                    layers_list.append(torch.nn.Dropout(drop_out))\n",
    "                if batch_norm:\n",
    "                    layers_list.append(torch.nn.BatchNorm1d(layers[Layer_i]))\n",
    "\n",
    "        #if the layer i is the last layer - add a sog-softmax \n",
    "        layers_list.append(torch.nn.LogSoftmax())\n",
    "        \n",
    "        self.layers_stack = torch.nn.Sequential(*layers_list) #this will enable getting params that can be trained\n",
    "        \n",
    "        self.layers = layers_list \n",
    "    def forward(self,x):        \n",
    "        x = x.view(-1,self.C_in,self.H_in,self.W_in) # flatten image input. This code will work for input as vector or image. will also work for batches. \n",
    "        batch_size = x.shape[0] \n",
    "\n",
    "        x = self.conv_layers_stack(x)\n",
    "        x = x.view(batch_size, -1) # flattern the \"image\". \n",
    "        logits = self.layers_stack(x)\n",
    "        \n",
    "        #for layer_index, layer in enumerate(self.layers): \n",
    "        #    x = self.layer(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0041540",
   "metadata": {},
   "source": [
    "## Defining the 3rd  model -   <font color = purple > Net models class with covolusion and max pool: </font> \n",
    "\n",
    "### This class enables defining a neural net model.\n",
    "\n",
    "The class needs to get as input:\n",
    "1. **input_layer** = the dimension of the input layer (insert as a list)\n",
    "2. **layers** = a list containing the dimensions  of the different layers ( without The input and output layers)\n",
    "3. **output_layer** =  the dimension of the output layer (insert as a list)\n",
    "\n",
    "\n",
    "4. **drop_out** = if you wish to include drop out in each hidden layer, then pass a float between 0 to 1. **default** = *False*\n",
    "5. **batch_norm** = if you wish to add a batch normalization layer. **default** = *False*\n",
    "\n",
    "\n",
    "Note: the input data has 3 dimensions - R G B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0ccf413",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T13:45:38.254317Z",
     "start_time": "2022-01-12T13:45:38.165468Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net_models_conv(\n",
      "  (conv_layers_stack): Sequential(\n",
      "    (0): Conv2d(3, 20, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(20, 4, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (layers_stack): Sequential(\n",
      "    (0): Linear(in_features=3844, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=10, bias=True)\n",
      "    (5): LogSoftmax(dim=None)\n",
      "  )\n",
      ")\n",
      "+----------------------------+------------+\n",
      "|          Modules           | Parameters |\n",
      "+----------------------------+------------+\n",
      "| conv_layers_stack.0.weight |    960     |\n",
      "|  conv_layers_stack.0.bias  |     20     |\n",
      "| conv_layers_stack.2.weight |    2000    |\n",
      "|  conv_layers_stack.2.bias  |     4      |\n",
      "|   layers_stack.0.weight    |  1968128   |\n",
      "|    layers_stack.0.bias     |    512     |\n",
      "|   layers_stack.2.weight    |   131072   |\n",
      "|    layers_stack.2.bias     |    256     |\n",
      "|   layers_stack.4.weight    |    2560    |\n",
      "|    layers_stack.4.bias     |     10     |\n",
      "+----------------------------+------------+\n",
      "Total Trainable Params: 2105522\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2105522"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_conv = Net_models_conv().to(device=device)\n",
    "print(model_conv)\n",
    "\n",
    "count_parameters(model_conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f9713407",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T13:45:39.853550Z",
     "start_time": "2022-01-12T13:45:39.808647Z"
    }
   },
   "outputs": [],
   "source": [
    "class Net_models_conv_maxpool(torch.nn.Module): # derive your model from the basic torch.nn.Model\n",
    "    def __init__(self, C_in = 3, H_in = 32, W_in = 32, layers=[512, 256], output_layer = [10],\n",
    "                 drop_out=False,\n",
    "                 batch_norm = False,\n",
    "                conv1_out_channels = 16, conv1_kernel_size = 5, conv1_padding = 2, maxpool1_kernel_size = 2,\n",
    "                conv2_out_channels = 4, conv2_kernel_size = 5, conv2_padding = 2, maxpool2_kernel_size = 2,\n",
    "                conv3_out_channels = 4, conv3_kernel_size = 5, conv3_padding = 2, maxpool3_kernel_size = 2):\n",
    "        super(Net_models_conv_maxpool,self).__init__()\n",
    "        self.C_in = C_in\n",
    "        self.H_in = H_in\n",
    "        self.W_in = W_in\n",
    "        \n",
    "        ######################## conv layer 1 - conv + relu + maxpool ###########################\n",
    "        conv1 = torch.nn.Conv2d(in_channels = C_in, #3\n",
    "                                     out_channels=conv1_out_channels,\n",
    "                                     kernel_size=conv1_kernel_size,\n",
    "                                     padding = conv1_padding) \n",
    "        batch_norm_1 = torch.nn.BatchNorm2d(conv1_out_channels)\n",
    "        relu1 = torch.nn.ReLU() \n",
    "        maxpool1 =  torch.nn.MaxPool2d(kernel_size=maxpool1_kernel_size)\n",
    "        out_channels_conv1, H_out_conv1, W_out_conv1 = conv_maxpool_output_shape(conv_layer = conv1,\n",
    "                                                                                 maxpoolayer = maxpool1,\n",
    "                                                                                 H_in = self.H_in,\n",
    "                                                                                 W_in = self.W_in, verbose = False)\n",
    "        ######################## conv layer 2 - conv + relu + maxpool ###########################\n",
    "        conv2 = torch.nn.Conv2d(in_channels= conv1.out_channels, #16\n",
    "                                     out_channels=conv2_out_channels,\n",
    "                                     kernel_size=conv2_kernel_size,\n",
    "                                     padding  = conv2_padding) \n",
    "        batch_norm_2 = torch.nn.BatchNorm2d(conv2_out_channels)\n",
    "        relu2 = torch.nn.ReLU()\n",
    "        maxpool2 =  torch.nn.MaxPool2d(kernel_size=maxpool2_kernel_size)\n",
    "        out_channels_conv2, H_out_conv2, W_out_conv2 = conv_maxpool_output_shape(conv_layer = conv2,\n",
    "                                                                                 maxpoolayer=maxpool2,\n",
    "                                                                                 H_in = H_out_conv1,\n",
    "                                                                                 W_in = W_out_conv1, verbose = False)\n",
    "        ######################## conv layer 3 - conv + relu + maxpool ###########################\n",
    "        conv3 = torch.nn.Conv2d(in_channels= conv2.out_channels, #16\n",
    "                                     out_channels=conv3_out_channels,\n",
    "                                     kernel_size=conv3_kernel_size,\n",
    "                                     padding  = conv3_padding) \n",
    "        batch_norm_3 = torch.nn.BatchNorm2d(conv3_out_channels)\n",
    "        relu3 = torch.nn.ReLU()\n",
    "        maxpool3 =  torch.nn.MaxPool2d(kernel_size=maxpool3_kernel_size)\n",
    "        out_channels_conv3, H_out_conv3, W_out_conv3 = conv_maxpool_output_shape(conv_layer = conv3,\n",
    "                                                                                 maxpoolayer=maxpool3,\n",
    "                                                                                 H_in = H_out_conv2,\n",
    "                                                                                 W_in = W_out_conv2, verbose = False)\n",
    "        ##################### combining all the con layers ##############################\n",
    "        conv_layers_list = [conv1,batch_norm_1, relu1, maxpool1, conv2, batch_norm_2, relu2, maxpool2,\n",
    "                           conv3, batch_norm_3, relu3, maxpool3]\n",
    "        self.conv_layers = conv_layers_list\n",
    "        self.conv_layers_stack = torch.nn.Sequential(*conv_layers_list) #this will enable getting params that can be trained\n",
    "        \n",
    "        #################################### fully connected layers (+relu) ###############\n",
    "        input_layer_for_fully_connected_layer_1 = [out_channels_conv3*H_out_conv3*W_out_conv3]\n",
    "        layers = input_layer_for_fully_connected_layer_1 + layers + output_layer # default: 3072x512x256x10        \n",
    "        \n",
    "        layers_list = []\n",
    "        for Layer_i in range(1, len(layers)): #1,2,3,... according to the number of layers given\n",
    "            \n",
    "            layers_list.append(torch.nn.Linear(layers[Layer_i-1], layers[Layer_i]) )\n",
    "\n",
    "            if Layer_i < len(layers)-1: #if the layer i is not the last layer - add a ReLU \n",
    "                layers_list.append( torch.nn.ReLU() )\n",
    "                if drop_out:\n",
    "                    layers_list.append(torch.nn.Dropout(drop_out))\n",
    "                if batch_norm:\n",
    "                    layers_list.append(torch.nn.BatchNorm1d(layers[Layer_i]))\n",
    "\n",
    "        #if the layer i is the last layer - add a sog-softmax \n",
    "        layers_list.append(torch.nn.LogSoftmax())\n",
    "        \n",
    "        self.layers_stack = torch.nn.Sequential(*layers_list) #this will enable getting params that can be trained\n",
    "        \n",
    "        self.layers = layers_list \n",
    "    def forward(self,x):        \n",
    "        x = x.view(-1,self.C_in,self.H_in,self.W_in) # flatten image input. This code will work for input as vector or image. will also work for batches. \n",
    "        batch_size = x.shape[0] \n",
    "\n",
    "        x = self.conv_layers_stack(x)\n",
    "        x = x.view(batch_size, -1) # flattern the \"image\". \n",
    "        logits = self.layers_stack(x)\n",
    "        \n",
    "        #for layer_index, layer in enumerate(self.layers): \n",
    "        #    x = self.layer(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b0d3385c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T13:45:42.574593Z",
     "start_time": "2022-01-12T13:45:42.536517Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net_models_conv_maxpool(\n",
      "  (conv_layers_stack): Sequential(\n",
      "    (0): Conv2d(3, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Conv2d(16, 4, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (5): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU()\n",
      "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (8): Conv2d(4, 4, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (9): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): ReLU()\n",
      "    (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layers_stack): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=10, bias=True)\n",
      "    (5): LogSoftmax(dim=None)\n",
      "  )\n",
      ")\n",
      "+----------------------------+------------+\n",
      "|          Modules           | Parameters |\n",
      "+----------------------------+------------+\n",
      "| conv_layers_stack.0.weight |    1200    |\n",
      "|  conv_layers_stack.0.bias  |     16     |\n",
      "| conv_layers_stack.1.weight |     16     |\n",
      "|  conv_layers_stack.1.bias  |     16     |\n",
      "| conv_layers_stack.4.weight |    1600    |\n",
      "|  conv_layers_stack.4.bias  |     4      |\n",
      "| conv_layers_stack.5.weight |     4      |\n",
      "|  conv_layers_stack.5.bias  |     4      |\n",
      "| conv_layers_stack.8.weight |    400     |\n",
      "|  conv_layers_stack.8.bias  |     4      |\n",
      "| conv_layers_stack.9.weight |     4      |\n",
      "|  conv_layers_stack.9.bias  |     4      |\n",
      "|   layers_stack.0.weight    |   32768    |\n",
      "|    layers_stack.0.bias     |    512     |\n",
      "|   layers_stack.2.weight    |   131072   |\n",
      "|    layers_stack.2.bias     |    256     |\n",
      "|   layers_stack.4.weight    |    2560    |\n",
      "|    layers_stack.4.bias     |     10     |\n",
      "+----------------------------+------------+\n",
      "Total Trainable Params: 170450\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "170450"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_conv_maxpool = Net_models_conv_maxpool().to(device=device)\n",
    "print(model_conv_maxpool)\n",
    "\n",
    "count_parameters(model_conv_maxpool)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fffcfe7",
   "metadata": {},
   "source": [
    "## Step 5 - Define loss funtion and optimizer\n",
    "In this step we need to define a loss function and an optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c7ef5ced",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T13:45:44.882650Z",
     "start_time": "2022-01-12T13:45:44.868979Z"
    }
   },
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss() #The loss function\n",
    "optimizer = torch.optim.SGD(model_conv.parameters(),lr = 0.01) #The optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a4b0f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-30T16:57:08.314307Z",
     "start_time": "2021-12-30T16:57:08.272322Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e14f72af",
   "metadata": {},
   "source": [
    "##  <font color = blue > Defining Model Accuracy function </font> \n",
    "This function will enable calculating a **classification model** performance - get its accuracy. It can be used on the training set, validation set and the test set.\n",
    "\n",
    "The function gets as input:\n",
    "1. **model** - The model object we wish to measure its accuracy\n",
    "2. **x_data** - torch.tensor containing the X matrix\n",
    "3. **y_data** - torch.tensor containing the labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d67741a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T13:45:46.082577Z",
     "start_time": "2022-01-12T13:45:46.072813Z"
    }
   },
   "outputs": [],
   "source": [
    "def model_accuracy(model, x_data, y_data):  \n",
    "    device = next(model.parameters()).device # get device on which the model is loaded. \n",
    "    model.eval() # must set model to evaluation mode to disable dropout!\n",
    "    with torch.no_grad():\n",
    "        #number_of_matches = 0\n",
    "        x_data, y_data = x_data.to(device=device), y_data.to(device=device)\n",
    "        _, max_index = model(x_data).max(dim=1)\n",
    "        number_of_matches = (max_index==y_data).to(dtype=torch.float32, device=torch.device(\"cpu\")).sum() \n",
    "        return number_of_matches/len(y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b0b597",
   "metadata": {},
   "source": [
    "## <font color = blue > Define Training function </font> Fot MLP - Multilayer perceptron \n",
    "### The function gets as input:\n",
    "\n",
    "1. neural net model, untrained\n",
    "2. x_train matrix (normalized and tensor type)\n",
    "3. y_train vector (normalized and tensor type)\n",
    "4. **Optimizer** - (The optimizer responsible to take the parameters (weights) that needs to be optimized and updates them according to the derivative\n",
    "5. **Criterion** - the loss function\n",
    "\n",
    "------------------------------- optional : -------------------------------\n",
    "\n",
    "6. **optional: x_validation_normalize_t** (x validation matrix - normalized and of type torch.tensor)\n",
    "7. **optional: y_validation_t** (y validation vector - normalized and of type torch.tensor)\n",
    "\n",
    "If *x_validation_normalize_t* is not provided, everything will be fine, you just won't get the validation accuracy score along the way.\n",
    "\n",
    "--------------------------------------------------------------\n",
    "8. n_epochs = the number of epochs, default = 50\n",
    "9. batch_size = the size of each batch in the training loop, default = 500\n",
    "10. device - use GPU  or CPU?, default = cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "51476af3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T13:45:47.352787Z",
     "start_time": "2022-01-12T13:45:47.320568Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_MLP_net_images(model, x_train_normalize_t, y_train_t, criterion, optimizer,\n",
    "                         x_validation_normalize_t = None, y_validation_t = None,\n",
    "                         n_epochs=50, batch_size=500, device=\"cpu\"):\n",
    "    ##########################################\n",
    "    train_images_norm = x_train_normalize_t.to(device=device)\n",
    "    train_labels = y_train_t.to(device=device)\n",
    "        \n",
    "    #checking if given validation set arguements\n",
    "    if x_validation_normalize_t is None or y_validation_t is None:\n",
    "        validation_set_checker = False\n",
    "    else:\n",
    "        validation_images = x_validation_normalize_t.to(device=device)\n",
    "        validation_labels = y_validation_t.to(device=device)\n",
    "        validation_set_checker = True\n",
    "    ##########################################\n",
    "    training_time_epochs = []\n",
    "    progress_log = []\n",
    "    model.to(device)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        model.train() # set model into train mode. \n",
    "        current_loss = 0\n",
    "        # prepare data for the current epoch (iterate over random indeces)\n",
    "        train_indeces = torch.randperm( (len(train_images_norm)//batch_size)*batch_size ).to(device=device) #  len(train_indeces) is an integer number of batches. \n",
    "        train_indeces = train_indeces.view(-1, batch_size) # shape: # batches x batch size, each *row* is a batch\n",
    "        \n",
    "        \n",
    "        training_time_batches = []\n",
    "        for current_batch_indeces in train_indeces: #for each row of indexes\n",
    "            train_data = train_images_norm[current_batch_indeces].to(dtype=torch.float32)\n",
    "            train_data_labels = train_labels[current_batch_indeces].to(dtype=torch.int64)\n",
    "            \n",
    "            optimizer.zero_grad() # clear gradients of all model parameters.\n",
    "            \n",
    "            start_train_time = time.time() #train time calculation\n",
    "            output = model(train_data) # forward pass: compute model predictions\n",
    "            batch_train_time = time.time() - start_train_time\n",
    "            training_time_batches.append(batch_train_time)\n",
    "            \n",
    "            loss = criterion(output, train_data_labels) # calculate the current loss\n",
    "            loss.backward() # backward pass: compute gradient of the loss for all model parameters. \n",
    "            optimizer.step() # use gradients to update the model parameters \n",
    "\n",
    "            current_loss += loss.item() * len(train_data) # weight loss by the size of the batch.\n",
    "            \n",
    "\n",
    "        training_time_epochs.append(np.mean(training_time_batches))\n",
    "        # evaluate performance of the batch\n",
    "        with torch.no_grad(): # disable autograd\n",
    "            train_accuracy = model_accuracy(model, train_data, train_data_labels)\n",
    "            if validation_set_checker:\n",
    "                validate_accuracy = model_accuracy(model, validation_images, validation_labels)\n",
    "            \n",
    "            if validation_set_checker:\n",
    "                print(f\"{epoch} of {n_epochs} | Loss: {current_loss/ len(train_images_norm):0.4f} | Train accuracy: {train_accuracy:0.4f} | Validate accuracy: {validate_accuracy:0.4f}\")\n",
    "            else:\n",
    "                print(f\"{epoch} of {n_epochs} | Loss: {current_loss/ len(train_images_norm):0.4f} | Train accuracy: {train_accuracy:0.4f}\")\n",
    "                validate_accuracy = None\n",
    "\n",
    "            progress_log.append({'epoch':epoch,\n",
    "                                 'loss':current_loss/len(train_images_norm),\n",
    "                                 'train epoch time': training_time_epochs[epoch],\n",
    "                                 'train_accuracy':train_accuracy, 'validate_accuracy':validate_accuracy })\n",
    "\n",
    "    print()\n",
    "    total_train_time = np.sum(training_time_epochs)\n",
    "    print(\"Total train time = \", total_train_time,\" seconds\")\n",
    "    print(\"Total train time = \", np.round(total_train_time/60,3),\" minutes\")\n",
    "    model.eval(); # set model mode to evaluation. \n",
    "    return model, progress_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "58fa4575",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T13:45:49.826831Z",
     "start_time": "2022-01-12T13:45:49.800462Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_MLP_net_images(model, x_train_normalize_t, y_train_t, criterion, optimizer,\n",
    "                         x_validation_normalize_t = None, y_validation_t = None,\n",
    "                         n_epochs=50, batch_size=500, device=\"cpu\"):\n",
    "    ##########################################\n",
    "    train_images_norm = x_train_normalize_t\n",
    "    train_labels = y_train_t\n",
    "        \n",
    "    #checking if given validation set arguements\n",
    "    if x_validation_normalize_t is None or y_validation_t is None:\n",
    "        validation_set_checker = False\n",
    "    else:\n",
    "        validation_images = x_validation_normalize_t.to(device=device)\n",
    "        validation_labels = y_validation_t.to(device=device)\n",
    "        validation_set_checker = True\n",
    "    ##########################################\n",
    "    training_time_epochs = []\n",
    "    progress_log = []\n",
    "    model.to(device)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        model.train() # set model into train mode. \n",
    "        current_loss = 0\n",
    "        # prepare data for the current epoch (iterate over random indeces)\n",
    "        train_indeces = torch.randperm( (len(train_images_norm)//batch_size)*batch_size ) #  len(train_indeces) is an integer number of batches. \n",
    "        train_indeces = train_indeces.view(-1, batch_size) # shape: # batches x batch size, each *row* is a batch\n",
    "        \n",
    "        \n",
    "        training_time_batches = []\n",
    "        for current_batch_indeces in train_indeces: #for each row of indexes\n",
    "            train_data = train_images_norm[current_batch_indeces].to(dtype=torch.float32).to(device=device)\n",
    "            train_data_labels = train_labels[current_batch_indeces].to(dtype=torch.int64).to(device=device)\n",
    "            \n",
    "            optimizer.zero_grad() # clear gradients of all model parameters.\n",
    "            \n",
    "            start_train_time = time.time() #train time calculation\n",
    "            output = model(train_data) # forward pass: compute model predictions\n",
    "            batch_train_time = time.time() - start_train_time\n",
    "            training_time_batches.append(batch_train_time)\n",
    "            \n",
    "            loss = criterion(output, train_data_labels) # calculate the current loss\n",
    "            loss.backward() # backward pass: compute gradient of the loss for all model parameters. \n",
    "            optimizer.step() # use gradients to update the model parameters \n",
    "\n",
    "            current_loss += loss.item() * len(train_data) # weight loss by the size of the batch.\n",
    "            \n",
    "\n",
    "        training_time_epochs.append(np.mean(training_time_batches))\n",
    "        # evaluate performance of the batch\n",
    "        with torch.no_grad(): # disable autograd\n",
    "            train_accuracy = model_accuracy(model, train_data, train_data_labels)\n",
    "            if validation_set_checker:\n",
    "                validate_accuracy = model_accuracy(model, validation_images, validation_labels)\n",
    "            \n",
    "            if validation_set_checker:\n",
    "                print(f\"{epoch} of {n_epochs} | Loss: {current_loss/ len(train_images_norm):0.4f} | Train accuracy: {train_accuracy:0.4f} | Validate accuracy: {validate_accuracy:0.4f}\")\n",
    "            else:\n",
    "                print(f\"{epoch} of {n_epochs} | Loss: {current_loss/ len(train_images_norm):0.4f} | Train accuracy: {train_accuracy:0.4f}\")\n",
    "                validate_accuracy = None\n",
    "\n",
    "            progress_log.append({'epoch':epoch,\n",
    "                                 'loss':current_loss/len(train_images_norm),\n",
    "                                 'train epoch time': training_time_epochs[epoch],\n",
    "                                 'train_accuracy':train_accuracy, 'validate_accuracy':validate_accuracy })\n",
    "\n",
    "    print()\n",
    "    total_train_time = np.sum(training_time_epochs)\n",
    "    print(\"Total train time = \", total_train_time,\" seconds\")\n",
    "    print(\"Total train time = \", np.round(total_train_time/60,3),\" minutes\")\n",
    "    model.eval(); # set model mode to evaluation. \n",
    "    return model, progress_log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56c9661",
   "metadata": {},
   "source": [
    "# **Calculating models performance**\n",
    "# ---------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836a0319",
   "metadata": {},
   "source": [
    "## Model Accuracy without convolution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7835c382",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-08T19:48:01.091698Z",
     "start_time": "2022-01-08T19:20:02.401552Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Roy\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py:141: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 of 100 | Loss: 1.9189 | Train accuracy: 0.3364 | Validate accuracy: 0.3392\n",
      "1 of 100 | Loss: 1.7514 | Train accuracy: 0.3727 | Validate accuracy: 0.3660\n",
      "2 of 100 | Loss: 1.7103 | Train accuracy: 0.4364 | Validate accuracy: 0.4104\n",
      "3 of 100 | Loss: 1.6734 | Train accuracy: 0.4382 | Validate accuracy: 0.3996\n",
      "4 of 100 | Loss: 1.6544 | Train accuracy: 0.4291 | Validate accuracy: 0.3852\n",
      "5 of 100 | Loss: 1.6469 | Train accuracy: 0.4527 | Validate accuracy: 0.4160\n",
      "6 of 100 | Loss: 1.6268 | Train accuracy: 0.4673 | Validate accuracy: 0.4204\n",
      "7 of 100 | Loss: 1.6209 | Train accuracy: 0.4873 | Validate accuracy: 0.4324\n",
      "8 of 100 | Loss: 1.5959 | Train accuracy: 0.4818 | Validate accuracy: 0.4260\n",
      "9 of 100 | Loss: 1.5843 | Train accuracy: 0.4236 | Validate accuracy: 0.4284\n",
      "10 of 100 | Loss: 1.5956 | Train accuracy: 0.4273 | Validate accuracy: 0.4164\n",
      "11 of 100 | Loss: 1.5756 | Train accuracy: 0.4582 | Validate accuracy: 0.4392\n",
      "12 of 100 | Loss: 1.5680 | Train accuracy: 0.5036 | Validate accuracy: 0.4580\n",
      "13 of 100 | Loss: 1.5461 | Train accuracy: 0.5127 | Validate accuracy: 0.4516\n",
      "14 of 100 | Loss: 1.5314 | Train accuracy: 0.4145 | Validate accuracy: 0.3936\n",
      "15 of 100 | Loss: 1.5287 | Train accuracy: 0.4818 | Validate accuracy: 0.4760\n",
      "16 of 100 | Loss: 1.5166 | Train accuracy: 0.4927 | Validate accuracy: 0.4708\n",
      "17 of 100 | Loss: 1.4931 | Train accuracy: 0.4582 | Validate accuracy: 0.4524\n",
      "18 of 100 | Loss: 1.4942 | Train accuracy: 0.5127 | Validate accuracy: 0.4592\n",
      "19 of 100 | Loss: 1.4792 | Train accuracy: 0.4927 | Validate accuracy: 0.4552\n",
      "20 of 100 | Loss: 1.4710 | Train accuracy: 0.5455 | Validate accuracy: 0.4668\n",
      "21 of 100 | Loss: 1.4622 | Train accuracy: 0.5382 | Validate accuracy: 0.4576\n",
      "22 of 100 | Loss: 1.4591 | Train accuracy: 0.5273 | Validate accuracy: 0.4880\n",
      "23 of 100 | Loss: 1.4515 | Train accuracy: 0.5345 | Validate accuracy: 0.4696\n",
      "24 of 100 | Loss: 1.4408 | Train accuracy: 0.5309 | Validate accuracy: 0.4960\n",
      "25 of 100 | Loss: 1.4404 | Train accuracy: 0.5382 | Validate accuracy: 0.4908\n",
      "26 of 100 | Loss: 1.4368 | Train accuracy: 0.4509 | Validate accuracy: 0.4424\n",
      "27 of 100 | Loss: 1.4340 | Train accuracy: 0.5127 | Validate accuracy: 0.4828\n",
      "28 of 100 | Loss: 1.4195 | Train accuracy: 0.5800 | Validate accuracy: 0.5000\n",
      "29 of 100 | Loss: 1.4169 | Train accuracy: 0.5691 | Validate accuracy: 0.4792\n",
      "30 of 100 | Loss: 1.4019 | Train accuracy: 0.5527 | Validate accuracy: 0.4880\n",
      "31 of 100 | Loss: 1.4013 | Train accuracy: 0.5455 | Validate accuracy: 0.4764\n",
      "32 of 100 | Loss: 1.3984 | Train accuracy: 0.5164 | Validate accuracy: 0.4824\n",
      "33 of 100 | Loss: 1.3887 | Train accuracy: 0.5218 | Validate accuracy: 0.4812\n",
      "34 of 100 | Loss: 1.3839 | Train accuracy: 0.5164 | Validate accuracy: 0.4792\n",
      "35 of 100 | Loss: 1.3808 | Train accuracy: 0.4582 | Validate accuracy: 0.4592\n",
      "36 of 100 | Loss: 1.3708 | Train accuracy: 0.5782 | Validate accuracy: 0.4868\n",
      "37 of 100 | Loss: 1.3713 | Train accuracy: 0.5745 | Validate accuracy: 0.4972\n",
      "38 of 100 | Loss: 1.3661 | Train accuracy: 0.5455 | Validate accuracy: 0.4892\n",
      "39 of 100 | Loss: 1.3610 | Train accuracy: 0.5891 | Validate accuracy: 0.5144\n",
      "40 of 100 | Loss: 1.3550 | Train accuracy: 0.5109 | Validate accuracy: 0.4676\n",
      "41 of 100 | Loss: 1.3526 | Train accuracy: 0.5345 | Validate accuracy: 0.4496\n",
      "42 of 100 | Loss: 1.3506 | Train accuracy: 0.5491 | Validate accuracy: 0.5016\n",
      "43 of 100 | Loss: 1.3459 | Train accuracy: 0.5764 | Validate accuracy: 0.5000\n",
      "44 of 100 | Loss: 1.3357 | Train accuracy: 0.5564 | Validate accuracy: 0.4988\n",
      "45 of 100 | Loss: 1.3355 | Train accuracy: 0.5273 | Validate accuracy: 0.5012\n",
      "46 of 100 | Loss: 1.3293 | Train accuracy: 0.6073 | Validate accuracy: 0.5116\n",
      "47 of 100 | Loss: 1.3247 | Train accuracy: 0.5945 | Validate accuracy: 0.5060\n",
      "48 of 100 | Loss: 1.3266 | Train accuracy: 0.5836 | Validate accuracy: 0.5088\n",
      "49 of 100 | Loss: 1.3212 | Train accuracy: 0.5527 | Validate accuracy: 0.5004\n",
      "50 of 100 | Loss: 1.3128 | Train accuracy: 0.5691 | Validate accuracy: 0.5024\n",
      "51 of 100 | Loss: 1.3103 | Train accuracy: 0.6255 | Validate accuracy: 0.5020\n",
      "52 of 100 | Loss: 1.3065 | Train accuracy: 0.5964 | Validate accuracy: 0.5052\n",
      "53 of 100 | Loss: 1.3059 | Train accuracy: 0.5564 | Validate accuracy: 0.4904\n",
      "54 of 100 | Loss: 1.3020 | Train accuracy: 0.5600 | Validate accuracy: 0.4844\n",
      "55 of 100 | Loss: 1.3009 | Train accuracy: 0.5455 | Validate accuracy: 0.5076\n",
      "56 of 100 | Loss: 1.2993 | Train accuracy: 0.5327 | Validate accuracy: 0.4932\n",
      "57 of 100 | Loss: 1.2934 | Train accuracy: 0.6055 | Validate accuracy: 0.4932\n",
      "58 of 100 | Loss: 1.2883 | Train accuracy: 0.6327 | Validate accuracy: 0.5080\n",
      "59 of 100 | Loss: 1.2855 | Train accuracy: 0.6218 | Validate accuracy: 0.5232\n",
      "60 of 100 | Loss: 1.2858 | Train accuracy: 0.5636 | Validate accuracy: 0.4812\n",
      "61 of 100 | Loss: 1.2794 | Train accuracy: 0.5873 | Validate accuracy: 0.5040\n",
      "62 of 100 | Loss: 1.2768 | Train accuracy: 0.4855 | Validate accuracy: 0.4632\n",
      "63 of 100 | Loss: 1.2741 | Train accuracy: 0.6127 | Validate accuracy: 0.5060\n",
      "64 of 100 | Loss: 1.2714 | Train accuracy: 0.5600 | Validate accuracy: 0.4840\n",
      "65 of 100 | Loss: 1.2678 | Train accuracy: 0.6127 | Validate accuracy: 0.5164\n",
      "66 of 100 | Loss: 1.2642 | Train accuracy: 0.5927 | Validate accuracy: 0.5128\n",
      "67 of 100 | Loss: 1.2574 | Train accuracy: 0.5764 | Validate accuracy: 0.4936\n",
      "68 of 100 | Loss: 1.2581 | Train accuracy: 0.6218 | Validate accuracy: 0.4940\n",
      "69 of 100 | Loss: 1.2561 | Train accuracy: 0.6145 | Validate accuracy: 0.5076\n",
      "70 of 100 | Loss: 1.2500 | Train accuracy: 0.5618 | Validate accuracy: 0.4928\n",
      "71 of 100 | Loss: 1.2481 | Train accuracy: 0.5291 | Validate accuracy: 0.4944\n",
      "72 of 100 | Loss: 1.2494 | Train accuracy: 0.5855 | Validate accuracy: 0.5172\n",
      "73 of 100 | Loss: 1.2445 | Train accuracy: 0.5891 | Validate accuracy: 0.5056\n",
      "74 of 100 | Loss: 1.2448 | Train accuracy: 0.6145 | Validate accuracy: 0.4904\n",
      "75 of 100 | Loss: 1.2363 | Train accuracy: 0.5782 | Validate accuracy: 0.4852\n",
      "76 of 100 | Loss: 1.2357 | Train accuracy: 0.5709 | Validate accuracy: 0.4948\n",
      "77 of 100 | Loss: 1.2319 | Train accuracy: 0.6073 | Validate accuracy: 0.5112\n",
      "78 of 100 | Loss: 1.2353 | Train accuracy: 0.6127 | Validate accuracy: 0.5116\n",
      "79 of 100 | Loss: 1.2283 | Train accuracy: 0.4727 | Validate accuracy: 0.4416\n",
      "80 of 100 | Loss: 1.2218 | Train accuracy: 0.6127 | Validate accuracy: 0.5080\n",
      "81 of 100 | Loss: 1.2282 | Train accuracy: 0.5982 | Validate accuracy: 0.5112\n",
      "82 of 100 | Loss: 1.2233 | Train accuracy: 0.6382 | Validate accuracy: 0.5244\n",
      "83 of 100 | Loss: 1.2221 | Train accuracy: 0.6418 | Validate accuracy: 0.5112\n",
      "84 of 100 | Loss: 1.2206 | Train accuracy: 0.6345 | Validate accuracy: 0.5016\n",
      "85 of 100 | Loss: 1.2174 | Train accuracy: 0.5927 | Validate accuracy: 0.4944\n",
      "86 of 100 | Loss: 1.2173 | Train accuracy: 0.6109 | Validate accuracy: 0.5164\n",
      "87 of 100 | Loss: 1.2142 | Train accuracy: 0.6273 | Validate accuracy: 0.5008\n",
      "88 of 100 | Loss: 1.2064 | Train accuracy: 0.6418 | Validate accuracy: 0.5120\n",
      "89 of 100 | Loss: 1.2053 | Train accuracy: 0.6527 | Validate accuracy: 0.5200\n",
      "90 of 100 | Loss: 1.1980 | Train accuracy: 0.5873 | Validate accuracy: 0.4824\n",
      "91 of 100 | Loss: 1.2042 | Train accuracy: 0.6600 | Validate accuracy: 0.5164\n",
      "92 of 100 | Loss: 1.2030 | Train accuracy: 0.5873 | Validate accuracy: 0.4916\n",
      "93 of 100 | Loss: 1.1921 | Train accuracy: 0.6255 | Validate accuracy: 0.5132\n",
      "94 of 100 | Loss: 1.1961 | Train accuracy: 0.5818 | Validate accuracy: 0.4676\n",
      "95 of 100 | Loss: 1.1870 | Train accuracy: 0.6091 | Validate accuracy: 0.4836\n",
      "96 of 100 | Loss: 1.1926 | Train accuracy: 0.6473 | Validate accuracy: 0.5072\n",
      "97 of 100 | Loss: 1.1909 | Train accuracy: 0.6055 | Validate accuracy: 0.4916\n",
      "98 of 100 | Loss: 1.1884 | Train accuracy: 0.5782 | Validate accuracy: 0.5116\n",
      "99 of 100 | Loss: 1.1851 | Train accuracy: 0.5927 | Validate accuracy: 0.4964\n",
      "\n",
      "Total train time =  7.844380398129307  seconds\n",
      "Total train time =  0.131  minutes\n"
     ]
    }
   ],
   "source": [
    "#all together:\n",
    "model = Net_models(layers=[650, 512, 256, 150], output_layer = [10],\n",
    "                             drop_out=0.2,\n",
    "                             batch_norm = True).to(device=device)\n",
    "criterion = torch.nn.CrossEntropyLoss() #The loss function\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr = 0.18) #The optimizer\n",
    "\n",
    "n_epochs = 100\n",
    "batch_size = 550\n",
    "\n",
    "model_new, progress_log = train_MLP_net_images(model = model,\n",
    "                                 x_train_normalize_t = X_train_t_normalize , y_train_t = y_train_t,\n",
    "                                 x_validation_normalize_t = X_validation_t_normalize, y_validation_t = y_validation_t,\n",
    "                                 criterion = criterion, optimizer = optimizer,\n",
    "                                 n_epochs = n_epochs, batch_size = batch_size, device = device)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4745abb",
   "metadata": {},
   "source": [
    "## Model Accuracy with convolution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "60369360",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-01T17:04:14.205749Z",
     "start_time": "2022-01-01T15:59:47.015486Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 of 60 | Loss: 2.1714 | Train accuracy: 0.3320 | Validate accuracy: 0.3028\n",
      "1 of 60 | Loss: 1.8748 | Train accuracy: 0.4160 | Validate accuracy: 0.3640\n",
      "2 of 60 | Loss: 1.7654 | Train accuracy: 0.4060 | Validate accuracy: 0.3884\n",
      "3 of 60 | Loss: 1.6947 | Train accuracy: 0.4480 | Validate accuracy: 0.4216\n",
      "4 of 60 | Loss: 1.6431 | Train accuracy: 0.4420 | Validate accuracy: 0.4388\n",
      "5 of 60 | Loss: 1.6048 | Train accuracy: 0.4800 | Validate accuracy: 0.4468\n",
      "6 of 60 | Loss: 1.5674 | Train accuracy: 0.4400 | Validate accuracy: 0.4344\n",
      "7 of 60 | Loss: 1.5396 | Train accuracy: 0.5160 | Validate accuracy: 0.4660\n",
      "8 of 60 | Loss: 1.5098 | Train accuracy: 0.4820 | Validate accuracy: 0.4524\n",
      "9 of 60 | Loss: 1.4822 | Train accuracy: 0.5260 | Validate accuracy: 0.4860\n",
      "10 of 60 | Loss: 1.4583 | Train accuracy: 0.5700 | Validate accuracy: 0.4880\n",
      "11 of 60 | Loss: 1.4354 | Train accuracy: 0.5420 | Validate accuracy: 0.5132\n",
      "12 of 60 | Loss: 1.4214 | Train accuracy: 0.5720 | Validate accuracy: 0.5024\n",
      "13 of 60 | Loss: 1.3882 | Train accuracy: 0.5920 | Validate accuracy: 0.5180\n",
      "14 of 60 | Loss: 1.3731 | Train accuracy: 0.6120 | Validate accuracy: 0.5116\n",
      "15 of 60 | Loss: 1.3495 | Train accuracy: 0.6020 | Validate accuracy: 0.5372\n",
      "16 of 60 | Loss: 1.3184 | Train accuracy: 0.5840 | Validate accuracy: 0.5208\n",
      "17 of 60 | Loss: 1.3065 | Train accuracy: 0.5620 | Validate accuracy: 0.5388\n",
      "18 of 60 | Loss: 1.2823 | Train accuracy: 0.5840 | Validate accuracy: 0.5020\n",
      "19 of 60 | Loss: 1.2573 | Train accuracy: 0.6120 | Validate accuracy: 0.4976\n",
      "20 of 60 | Loss: 1.2499 | Train accuracy: 0.6520 | Validate accuracy: 0.5292\n",
      "21 of 60 | Loss: 1.2208 | Train accuracy: 0.6920 | Validate accuracy: 0.5528\n",
      "22 of 60 | Loss: 1.1979 | Train accuracy: 0.6040 | Validate accuracy: 0.4976\n",
      "23 of 60 | Loss: 1.1804 | Train accuracy: 0.6380 | Validate accuracy: 0.5172\n",
      "24 of 60 | Loss: 1.1699 | Train accuracy: 0.6820 | Validate accuracy: 0.5136\n",
      "25 of 60 | Loss: 1.1384 | Train accuracy: 0.7020 | Validate accuracy: 0.5212\n",
      "26 of 60 | Loss: 1.1290 | Train accuracy: 0.6920 | Validate accuracy: 0.5360\n",
      "27 of 60 | Loss: 1.0954 | Train accuracy: 0.7400 | Validate accuracy: 0.5388\n",
      "28 of 60 | Loss: 1.0870 | Train accuracy: 0.7620 | Validate accuracy: 0.5444\n",
      "29 of 60 | Loss: 1.0578 | Train accuracy: 0.6980 | Validate accuracy: 0.5288\n",
      "30 of 60 | Loss: 1.0336 | Train accuracy: 0.7560 | Validate accuracy: 0.5264\n",
      "31 of 60 | Loss: 1.0153 | Train accuracy: 0.7000 | Validate accuracy: 0.5216\n",
      "32 of 60 | Loss: 1.0106 | Train accuracy: 0.7560 | Validate accuracy: 0.5444\n",
      "33 of 60 | Loss: 0.9819 | Train accuracy: 0.7720 | Validate accuracy: 0.5172\n",
      "34 of 60 | Loss: 0.9512 | Train accuracy: 0.7680 | Validate accuracy: 0.5456\n",
      "35 of 60 | Loss: 0.9446 | Train accuracy: 0.7780 | Validate accuracy: 0.5240\n",
      "36 of 60 | Loss: 0.9217 | Train accuracy: 0.8320 | Validate accuracy: 0.5272\n",
      "37 of 60 | Loss: 0.9010 | Train accuracy: 0.7720 | Validate accuracy: 0.5312\n",
      "38 of 60 | Loss: 0.8791 | Train accuracy: 0.8240 | Validate accuracy: 0.5304\n",
      "39 of 60 | Loss: 0.8600 | Train accuracy: 0.8280 | Validate accuracy: 0.5200\n",
      "40 of 60 | Loss: 0.8464 | Train accuracy: 0.7840 | Validate accuracy: 0.5284\n",
      "41 of 60 | Loss: 0.8265 | Train accuracy: 0.8400 | Validate accuracy: 0.5380\n",
      "42 of 60 | Loss: 0.7995 | Train accuracy: 0.8520 | Validate accuracy: 0.5028\n",
      "43 of 60 | Loss: 0.7788 | Train accuracy: 0.8600 | Validate accuracy: 0.5392\n",
      "44 of 60 | Loss: 0.7615 | Train accuracy: 0.8240 | Validate accuracy: 0.5196\n",
      "45 of 60 | Loss: 0.7464 | Train accuracy: 0.8460 | Validate accuracy: 0.5212\n",
      "46 of 60 | Loss: 0.7359 | Train accuracy: 0.8100 | Validate accuracy: 0.5152\n",
      "47 of 60 | Loss: 0.7129 | Train accuracy: 0.8400 | Validate accuracy: 0.5280\n",
      "48 of 60 | Loss: 0.7007 | Train accuracy: 0.9000 | Validate accuracy: 0.5304\n",
      "49 of 60 | Loss: 0.6690 | Train accuracy: 0.8600 | Validate accuracy: 0.5304\n",
      "50 of 60 | Loss: 0.6685 | Train accuracy: 0.9160 | Validate accuracy: 0.5252\n",
      "51 of 60 | Loss: 0.6476 | Train accuracy: 0.5720 | Validate accuracy: 0.3700\n",
      "52 of 60 | Loss: 0.6297 | Train accuracy: 0.9060 | Validate accuracy: 0.5284\n",
      "53 of 60 | Loss: 0.6133 | Train accuracy: 0.8840 | Validate accuracy: 0.5168\n",
      "54 of 60 | Loss: 0.5954 | Train accuracy: 0.8960 | Validate accuracy: 0.5272\n",
      "55 of 60 | Loss: 0.5829 | Train accuracy: 0.9300 | Validate accuracy: 0.5168\n",
      "56 of 60 | Loss: 0.5806 | Train accuracy: 0.9380 | Validate accuracy: 0.5120\n",
      "57 of 60 | Loss: 0.5661 | Train accuracy: 0.9180 | Validate accuracy: 0.5164\n",
      "58 of 60 | Loss: 0.5418 | Train accuracy: 0.9200 | Validate accuracy: 0.5096\n",
      "59 of 60 | Loss: 0.5577 | Train accuracy: 0.9240 | Validate accuracy: 0.5100\n",
      "\n",
      "Total train time =  15.875776807885421  seconds\n",
      "Total train time =  0.265  minutes\n"
     ]
    }
   ],
   "source": [
    "#all together:\n",
    "model_conv = Net_models_conv(C_in = 3, H_in = 32, W_in = 32,\n",
    "                             layers=[512, 256, 150, 100], output_layer = [10],\n",
    "                             drop_out=0.3,\n",
    "                             batch_norm = True).to(device=device)\n",
    "criterion = torch.nn.CrossEntropyLoss() #The loss function\n",
    "optimizer = torch.optim.SGD(model_conv.parameters(),lr = 0.13) #The optimizer\n",
    "\n",
    "n_epochs = 60\n",
    "batch_size = 500\n",
    "\n",
    "model_new, progress_log = train_MLP_net_images(model = model_conv,\n",
    "                                 x_train_normalize_t = X_train_t_normalize , y_train_t = y_train_t,\n",
    "                                 x_validation_normalize_t = X_validation_t_normalize, y_validation_t = y_validation_t,\n",
    "                                 criterion = criterion, optimizer = optimizer,\n",
    "                                 n_epochs = n_epochs, batch_size = batch_size, device = device)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563b992d",
   "metadata": {},
   "source": [
    "## Model Accuracy with convolution and with max-pool:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "304022ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 of 50 | Loss: 1.9433 | Train accuracy: 0.2771 | Validate accuracy: 0.2440\n",
      "1 of 50 | Loss: 1.7343 | Train accuracy: 0.3586 | Validate accuracy: 0.3472\n",
      "2 of 50 | Loss: 1.6143 | Train accuracy: 0.3314 | Validate accuracy: 0.3224\n",
      "3 of 50 | Loss: 1.5450 | Train accuracy: 0.3686 | Validate accuracy: 0.3468\n",
      "4 of 50 | Loss: 1.5019 | Train accuracy: 0.4386 | Validate accuracy: 0.3884\n",
      "5 of 50 | Loss: 1.4560 | Train accuracy: 0.4100 | Validate accuracy: 0.3900\n",
      "6 of 50 | Loss: 1.4230 | Train accuracy: 0.4171 | Validate accuracy: 0.4008\n",
      "7 of 50 | Loss: 1.3962 | Train accuracy: 0.4386 | Validate accuracy: 0.4164\n",
      "8 of 50 | Loss: 1.3642 | Train accuracy: 0.3857 | Validate accuracy: 0.3756\n",
      "9 of 50 | Loss: 1.3356 | Train accuracy: 0.4957 | Validate accuracy: 0.4612\n",
      "10 of 50 | Loss: 1.3105 | Train accuracy: 0.4500 | Validate accuracy: 0.4204\n",
      "11 of 50 | Loss: 1.2879 | Train accuracy: 0.5300 | Validate accuracy: 0.5084\n",
      "12 of 50 | Loss: 1.2662 | Train accuracy: 0.5014 | Validate accuracy: 0.4740\n",
      "13 of 50 | Loss: 1.2401 | Train accuracy: 0.5000 | Validate accuracy: 0.4528\n",
      "14 of 50 | Loss: 1.2129 | Train accuracy: 0.5129 | Validate accuracy: 0.4716\n",
      "15 of 50 | Loss: 1.2011 | Train accuracy: 0.5300 | Validate accuracy: 0.4496\n",
      "16 of 50 | Loss: 1.1811 | Train accuracy: 0.5886 | Validate accuracy: 0.5316\n",
      "17 of 50 | Loss: 1.1768 | Train accuracy: 0.5871 | Validate accuracy: 0.5096\n",
      "18 of 50 | Loss: 1.1568 | Train accuracy: 0.5514 | Validate accuracy: 0.4840\n",
      "19 of 50 | Loss: 1.1373 | Train accuracy: 0.5300 | Validate accuracy: 0.4876\n",
      "20 of 50 | Loss: 1.1184 | Train accuracy: 0.5471 | Validate accuracy: 0.5108\n",
      "21 of 50 | Loss: 1.1068 | Train accuracy: 0.5886 | Validate accuracy: 0.5228\n",
      "22 of 50 | Loss: 1.1033 | Train accuracy: 0.5629 | Validate accuracy: 0.5124\n",
      "23 of 50 | Loss: 1.0845 | Train accuracy: 0.6271 | Validate accuracy: 0.5336\n",
      "24 of 50 | Loss: 1.0619 | Train accuracy: 0.6229 | Validate accuracy: 0.5612\n",
      "25 of 50 | Loss: 1.0516 | Train accuracy: 0.5786 | Validate accuracy: 0.5152\n",
      "26 of 50 | Loss: 1.0446 | Train accuracy: 0.5057 | Validate accuracy: 0.4692\n",
      "27 of 50 | Loss: 1.0328 | Train accuracy: 0.5529 | Validate accuracy: 0.4836\n",
      "28 of 50 | Loss: 1.0166 | Train accuracy: 0.6000 | Validate accuracy: 0.5300\n",
      "29 of 50 | Loss: 1.0052 | Train accuracy: 0.6171 | Validate accuracy: 0.5308\n",
      "30 of 50 | Loss: 1.0005 | Train accuracy: 0.6243 | Validate accuracy: 0.5220\n",
      "31 of 50 | Loss: 0.9816 | Train accuracy: 0.6043 | Validate accuracy: 0.5332\n",
      "32 of 50 | Loss: 0.9689 | Train accuracy: 0.6743 | Validate accuracy: 0.5568\n",
      "33 of 50 | Loss: 0.9638 | Train accuracy: 0.5857 | Validate accuracy: 0.5028\n",
      "34 of 50 | Loss: 0.9505 | Train accuracy: 0.6243 | Validate accuracy: 0.5356\n",
      "35 of 50 | Loss: 0.9411 | Train accuracy: 0.6229 | Validate accuracy: 0.5292\n",
      "36 of 50 | Loss: 0.9378 | Train accuracy: 0.6329 | Validate accuracy: 0.5340\n",
      "37 of 50 | Loss: 0.9148 | Train accuracy: 0.6357 | Validate accuracy: 0.4992\n",
      "38 of 50 | Loss: 0.9056 | Train accuracy: 0.6614 | Validate accuracy: 0.5160\n",
      "39 of 50 | Loss: 0.9022 | Train accuracy: 0.6657 | Validate accuracy: 0.5324\n",
      "40 of 50 | Loss: 0.8887 | Train accuracy: 0.7171 | Validate accuracy: 0.5448\n",
      "41 of 50 | Loss: 0.8812 | Train accuracy: 0.6314 | Validate accuracy: 0.5096\n",
      "42 of 50 | Loss: 0.8677 | Train accuracy: 0.6586 | Validate accuracy: 0.5180\n",
      "43 of 50 | Loss: 0.8589 | Train accuracy: 0.6071 | Validate accuracy: 0.5116\n",
      "44 of 50 | Loss: 0.8575 | Train accuracy: 0.6414 | Validate accuracy: 0.5204\n",
      "45 of 50 | Loss: 0.8444 | Train accuracy: 0.6057 | Validate accuracy: 0.5048\n",
      "46 of 50 | Loss: 0.8340 | Train accuracy: 0.6857 | Validate accuracy: 0.5368\n",
      "47 of 50 | Loss: 0.8287 | Train accuracy: 0.7157 | Validate accuracy: 0.5240\n",
      "48 of 50 | Loss: 0.8219 | Train accuracy: 0.5843 | Validate accuracy: 0.4964\n",
      "49 of 50 | Loss: 0.8057 | Train accuracy: 0.7129 | Validate accuracy: 0.5520\n",
      "\n",
      "Total train time =  0.10091338940520786  seconds\n",
      "Total train time =  0.002  minutes\n",
      "total time =  2 3 min\n"
     ]
    }
   ],
   "source": [
    "#all together:\n",
    "start_time = time.time()\n",
    "model_conv_pool = Net_models_conv_maxpool(C_in = 3, H_in = 32, W_in = 32,\n",
    "                layers=[600, 450, 300, 100], output_layer = [10],\n",
    "                 drop_out = 0.2,\n",
    "                 batch_norm = True,\n",
    "                conv1_out_channels = 30, conv1_kernel_size = 3, conv1_padding = 2, maxpool1_kernel_size = 2,\n",
    "                conv2_out_channels = 20, conv2_kernel_size = 5, conv2_padding = 2, maxpool2_kernel_size = 2,\n",
    "                conv3_out_channels = 10, conv3_kernel_size = 5, conv3_padding = 2, maxpool3_kernel_size = 2).to(device=device)\n",
    "                \n",
    "criterion = torch.nn.CrossEntropyLoss() #The loss function\n",
    "optimizer = torch.optim.SGD(model_conv_pool.parameters(),lr = 0.65, momentum=0.7) #The optimizer\n",
    "\n",
    "n_epochs = 50\n",
    "batch_size = 700\n",
    "\n",
    "model_new, progress_log = train_MLP_net_images(model = model_conv_pool,\n",
    "                                 x_train_normalize_t = X_train_t_normalize , y_train_t = y_train_t,\n",
    "                                 x_validation_normalize_t = X_validation_t_normalize, y_validation_t = y_validation_t,\n",
    "                                 criterion = criterion, optimizer = optimizer,\n",
    "                                 n_epochs = n_epochs, batch_size = batch_size, device = device)\n",
    "\n",
    "print(\"total time = \", round((time.time()-start_time)/60),3, \"min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f899f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "61d12124",
   "metadata": {},
   "source": [
    "## Summarizing models performance on train, validation and test sets:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0745699a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-01T18:23:23.132958Z",
     "start_time": "2022-01-01T18:16:49.588040Z"
    }
   },
   "outputs": [],
   "source": [
    "models = [model, model_conv, model_conv_pool]\n",
    "\n",
    "train_accuracy_list = []\n",
    "validate_accuracy_list = []\n",
    "test_accuracy_list = []\n",
    "testing_time_list = []\n",
    "\n",
    "train_images_norm = X_train_t_normalize.to(device=device)\n",
    "train_labels = y_train_t.to(device=device)\n",
    "validation_images = X_validation_t_normalize.to(device=device)\n",
    "validation_labels = y_validation_t.to(device=device)\n",
    "\n",
    "for model0 in models:\n",
    "\n",
    "    train_accuracy = model_accuracy(model0, train_images_norm, train_labels)\n",
    "    validate_accuracy = model_accuracy(model0, validation_images, validation_labels)  \n",
    "\n",
    "    start_testing_time = time.time() #measure the testing time\n",
    "    test_accuracy = model_accuracy(model0, X_test_t_normalize, y_test_t)  \n",
    "    testing_time_i = time.time() - start_testing_time #measure the testing time\n",
    "\n",
    "    train_accuracy_list.append(train_accuracy)\n",
    "    validate_accuracy_list.append(validate_accuracy)\n",
    "    test_accuracy_list.append(test_accuracy)\n",
    "    testing_time_list.append(testing_time_i)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1c6c70ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-01T18:23:23.681654Z",
     "start_time": "2022-01-01T18:23:23.186655Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>number of paramaters</th>\n",
       "      <th>Train Accuracy</th>\n",
       "      <th>Validation Accuracy</th>\n",
       "      <th>Test Accuracy</th>\n",
       "      <th>Test time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>model 1</th>\n",
       "      <td>2505286</td>\n",
       "      <td>0.596126</td>\n",
       "      <td>0.5048</td>\n",
       "      <td>0.4966</td>\n",
       "      <td>1.103254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model 2</th>\n",
       "      <td>2159648</td>\n",
       "      <td>0.903411</td>\n",
       "      <td>0.5100</td>\n",
       "      <td>0.5059</td>\n",
       "      <td>7.373261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model 3</th>\n",
       "      <td>540960</td>\n",
       "      <td>0.728147</td>\n",
       "      <td>0.5400</td>\n",
       "      <td>0.5372</td>\n",
       "      <td>7.039357</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         number of paramaters  Train Accuracy  Validation Accuracy  \\\n",
       "model 1               2505286        0.596126               0.5048   \n",
       "model 2               2159648        0.903411               0.5100   \n",
       "model 3                540960        0.728147               0.5400   \n",
       "\n",
       "         Test Accuracy  Test time  \n",
       "model 1         0.4966   1.103254  \n",
       "model 2         0.5059   7.373261  \n",
       "model 3         0.5372   7.039357  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_params_list = []\n",
    "model_names_list = []\n",
    "for i in range(len(models)):\n",
    "    total_params_list.append(sum(p.numel() for p in models[i].parameters()))\n",
    "    model_name_i = \"model \" + str(i+1)\n",
    "    model_names_list.append(model_name_i)\n",
    "\n",
    "df_summary_all = pd.DataFrame({\"number of paramaters\" : total_params_list,\n",
    "                               \"Train Accuracy\" : np.array(train_accuracy_list),\n",
    "                               \"Validation Accuracy\" : np.array(validate_accuracy_list),\n",
    "                               \"Test Accuracy\" : np.array(test_accuracy_list),\n",
    "                               \"Test time\" : testing_time_list\n",
    "                               },\n",
    "                              index = [model_names_list])\n",
    "df_summary_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9fa1ce",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "### We can see that the best performing model is the third one (the model that uses convolusion and maxpooling). It also has the lowest amount of parameters. This makes sense as we know that CNN or ConvNet, is a class of neural networks that specializes in processing data that has a grid-like topology, such as an image. We also know that a digital image is a binary representation of visual data that contains a series of pixels arranged in a grid-like fashion that contains pixel values to denote how bright and what color each pixel should be. The low number of parameters is due to the max pooling that decreased the parameters. Max-pooling's objective is to down-sample the image's representation, reducing its dimensionality and allowing for assumptions to be made about features contained in sub-regions of the image. Accordingly, these methods allow to prevent overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fc72fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1b453e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1601f2d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b782bc65",
   "metadata": {},
   "source": [
    "# <font color = crimson > Question 1-b : </font> Apply **data augmentation techniques**\n",
    "\n",
    "It is possible that the relatively small size of the dataset limits performance of your algorithm and/or measurements of its accuracy (due to small size of the test set).\n",
    "\n",
    "* Use **data augmentation** to generate **variations of images** thus **increasing the size of the available data**.\n",
    "* Use few augmenting transforms e.g. :\n",
    "    - ColorJitter\n",
    "    - crop and resize\n",
    "    - flip\n",
    "    - and other random transforms, along, perhaps with RandomChoice\n",
    "    \n",
    "to alter images. Take a look at **Image Augmentation.ipynb notebook** attached to this assignment for demonstration.\n",
    "\n",
    "* How models you have developed in part a) of this assignment perform with augmented images? Compare accuracy in two scenarios: with and without re-training the models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d6d03b",
   "metadata": {},
   "source": [
    "## <font color = blue >  Fucntion image_transform: transform the input image into 9 different augmented images </font>\n",
    "\n",
    "The function gets as input:\n",
    "1. **img** - the image must be of type torch.tensor and dtype - int between 0-255 or float between 0 and 1.\n",
    "2. **transform** - A torchvision.transforms object\n",
    "3. **random_state** - if you want the transformation to be replicatable - use a random state\n",
    "\n",
    "The function returns 9 images made by the tranformation on the initial image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "55842ed8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T13:46:01.990531Z",
     "start_time": "2022-01-12T13:46:01.966121Z"
    }
   },
   "outputs": [],
   "source": [
    "def image_transform(img, transform, random_state = None):\n",
    "    \n",
    "    if random_state:\n",
    "        torch.manual_seed(random_state)\n",
    "\n",
    "    #dealing with shape issue - we need to save it so we can later reshpae the transformed image\n",
    "    initial_shape = img.shape # 3 X H X W\n",
    "    image_c_shape = initial_shape[0] # c = 3\n",
    "    image_H_shape = initial_shape[1]\n",
    "    image_W_shape = initial_shape[2]\n",
    "    \n",
    "    %matplotlib inline\n",
    "    import matplotlib.pyplot as plt\n",
    "    fig, ax = plt.subplots(3,3, figsize=(12,9))\n",
    "    for i in range(3):\n",
    "        for j in range(3):\n",
    "            imgae_transformed = transform(img) #shape = 3 X H X W\n",
    "            image_transformed_reshape = imgae_transformed.reshape(image_H_shape, image_W_shape, image_c_shape)\n",
    "            \n",
    "            ax[i][j].imshow(image_transformed_reshape, interpolation='nearest',cmap='gray')\n",
    "            ax[i][j].axis('off')\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd714341",
   "metadata": {},
   "source": [
    "## <font color = blue > Fucntion create_augemnted_data : </font>  \n",
    "This function gets as input Images and labels (in torch tensor int32 type) and returns the same data + transformation\n",
    "\n",
    "+ The function gets as **input**:\n",
    "1. **images** - images must be of type torch.tensor and dtype - int32 (between 0-255) or float (between 0 and 1).\n",
    "2. **labels** - The given images labels (so the function will be able to create a augmented tensor of the labels). The labels needs to already be in torch tensor format.\n",
    "3. **transform** - A torchvision.transforms object, according to it the transformation will be done\n",
    "4. **num_of_replicates** - The number of transformed replication to make for each given image. By default = **10**\n",
    "5. **random_state** - if you want the transformation to be replicable - use a random state. By default = **None**\n",
    "6. **verbose** - if True, the function will print the number of **new** images that were created and the **total** number of images that are returned by the function. By default = **False**\n",
    "7. **return_the_initial_images** - by default = **True** - this will make the function to return the given data + the augmented transformed data. If **False** then the function won't return the initial given data, and only return the augmented transformed data. This option can be beneficial if you wish to use a couple of transformation on the given data and at the end concatenate all the augmented transformed data (so you can't have the initial given data repeated in each one of them).\n",
    "\n",
    "+ The function **returns** 2 things:\n",
    "1. **The augmented data**\n",
    "2. **The augmented data labels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "22996a59",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T13:46:03.591829Z",
     "start_time": "2022-01-12T13:46:03.581089Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_augemnted_data(images, labels, tranform, num_of_replicates = 10, random_state = None, verbose = False,\n",
    "                         return_the_initial_images = True):\n",
    "    if random_state:\n",
    "        torch.manual_seed(random_state)\n",
    "        \n",
    "    num_given_images = images.shape[0] #the umber of images inserted as input\n",
    "    augmented_data = images\n",
    "    augmented_labels = labels\n",
    "    \n",
    "    for i in range(num_of_replicates):\n",
    "        new_images = tranform(images)\n",
    "        augmented_data =  torch.cat((augmented_data, new_images), dim = 0) # add the new transformed images\n",
    "        augmented_labels = torch.cat((augmented_labels, labels), dim = 0) #add the labels of the new transformed images\n",
    "    \n",
    "    if return_the_initial_images is False:\n",
    "        augmented_data = augmented_data[images.shape[0]:]\n",
    "        augmented_labels = augmented_labels[labels.shape[0]:]\n",
    "        print(\"Not returning the initial given images\")\n",
    "        print(\"------------------------------------------------------\")\n",
    "        num_given_images = 0\n",
    "        \n",
    "    if verbose:\n",
    "        print(\"Number of new images created = \", augmented_data.shape[0] - num_given_images)\n",
    "        print(\"Total Number of images in output = \", augmented_data.shape[0])\n",
    "        \n",
    "    return augmented_data, augmented_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ff08fc",
   "metadata": {},
   "source": [
    "## Creating different transfomations to choose from randomly in the following sections:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1411c15b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T13:46:05.705293Z",
     "start_time": "2022-01-12T13:46:05.664289Z"
    }
   },
   "outputs": [],
   "source": [
    "#### optional transformations\n",
    "\n",
    "#ColorJitter = Randomly change the brightness, contrast, saturation and hue of an image.\n",
    "tr_ColorJitter_1 = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.ColorJitter(brightness=10, contrast=0, saturation=0, hue=0)\n",
    "    ])\n",
    "\n",
    "tr_ColorJitter_2 = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.ColorJitter(brightness=0, contrast=10, saturation=0, hue=0)\n",
    "    ])\n",
    "\n",
    "tr_ColorJitter_3 = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.ColorJitter(brightness=0, contrast=0, saturation=10, hue=0)\n",
    "    ])\n",
    "\n",
    "tr_ColorJitter_4 = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.ColorJitter(brightness=0, contrast=0, saturation=0, hue=0.5)\n",
    "    ])\n",
    "\n",
    "tr_ColorJitter_5 = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.ColorJitter(brightness=5, contrast=5, saturation=5, hue=0.5)\n",
    "    ])\n",
    "\n",
    "\n",
    "#Random crop\n",
    "#FiveCrop\n",
    "\n",
    "#Rotate / Shift / Scale - Random affine transformation of the image keeping center invariant\n",
    "tr_RandomAffine_1 = torchvision.transforms.Compose([\n",
    "                                torchvision.transforms.RandomAffine(degrees=(-45,45))\n",
    "        ])\n",
    "\n",
    "#Resize / scale / ratio - Crop a random portion of image and resize it to a given size.\n",
    "### size = (32,32) as this is the dimention of the images in the data\n",
    "tr_RandomResizedCrop_1 = torchvision.transforms.RandomResizedCrop(size=(32,32), scale =(0.1,2), ratio =(0.75, 1.4))\n",
    "\n",
    "#### horizontal flip + vertical flip + GaussianBlur\n",
    "tr_flip_blur_1 = torchvision.transforms.Compose(\n",
    "    [\n",
    "        torchvision.transforms.RandomHorizontalFlip(p=0.5),\n",
    "        torchvision.transforms.RandomVerticalFlip(p=0.5),\n",
    "        torchvision.transforms.RandomApply([ \n",
    "            torchvision.transforms.GaussianBlur(kernel_size=5, sigma=(0.1, 2.0))     ],\n",
    "                p=0.5)\n",
    "    ])\n",
    "        \n",
    "#image_transform(one_image_t, tr_flip_blur_1, random_state = 1998) ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1bac0e62",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T13:46:08.137639Z",
     "start_time": "2022-01-12T13:46:08.125926Z"
    }
   },
   "outputs": [],
   "source": [
    "tr = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.RandomChoice(\n",
    "    [torchvision.transforms.ColorJitter(brightness=10, contrast=0, saturation=0, hue=0),\n",
    "     torchvision.transforms.ColorJitter(brightness=0, contrast=10, saturation=0, hue=0),\n",
    "     torchvision.transforms.ColorJitter(brightness=0, contrast=0, saturation=10, hue=0),\n",
    "     torchvision.transforms.ColorJitter(brightness=0, contrast=0, saturation=0, hue=0.5),\n",
    "     torchvision.transforms.ColorJitter(brightness=5, contrast=5, saturation=5, hue=0.5),\n",
    "     torchvision.transforms.RandomAffine(degrees=(-45,45)),\n",
    "     torchvision.transforms.RandomResizedCrop(size=(32,32), scale =(0.1,2), ratio =(0.75, 1.4)),\n",
    "     tr_flip_blur_1\n",
    "\n",
    "])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7d06bfa0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-08T17:59:55.758540Z",
     "start_time": "2022-01-08T17:59:54.196647Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "#cifar_train_augmented = torchvision.datasets.CIFAR10(root=\"./data\", train = True, download = True, transform = tr)\n",
    "#cifar_test = torchvision.datasets.CIFAR10(root=\"./data\", train = False, download = True)\n",
    "\n",
    "#cifar_train_augmented.data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0726594",
   "metadata": {},
   "source": [
    "### Checking the function create_augemnted_data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c0baed1f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-08T17:53:07.973607Z",
     "start_time": "2022-01-08T17:53:06.260584Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not returning the initial given images\n",
      "------------------------------------------------------\n",
      "Number of new images created =  5000\n",
      "Total Number of images in output =  5000\n",
      "\n",
      "torch.Size([5000, 3, 32, 32])\n",
      "torch.Size([5000])\n",
      "tensor(6, dtype=torch.uint8) | tensor(6, dtype=torch.uint8)\n",
      "tensor(8, dtype=torch.uint8) | tensor(8, dtype=torch.uint8)\n",
      "tensor(4, dtype=torch.uint8) | tensor(4, dtype=torch.uint8)\n",
      "tensor(2, dtype=torch.uint8) | tensor(2, dtype=torch.uint8)\n",
      "tensor(7, dtype=torch.uint8) | tensor(7, dtype=torch.uint8)\n"
     ]
    }
   ],
   "source": [
    "### check the function create_augemnted_data\n",
    "some_images_t = X_train_t[0:100]\n",
    "some_y_train_t = y_train_t[0:100]\n",
    "num_of_replicates = 5\n",
    "new_images, new_labels = create_augemnted_data(some_images_t, some_y_train_t, tr, num_of_replicates=50, random_state=1998, verbose=True,\n",
    "                                              return_the_initial_images=False)\n",
    "print()\n",
    "print(new_images.shape)\n",
    "print(new_labels.shape)\n",
    "\n",
    "print(new_labels[0] ,\"|\", new_labels[100] )\n",
    "print(new_labels[1] ,\"|\", new_labels[101] )\n",
    "print(new_labels[2] ,\"|\", new_labels[102] )\n",
    "print(new_labels[3] ,\"|\", new_labels[103] )\n",
    "print(new_labels[99] ,\"|\", new_labels[199] )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4889066",
   "metadata": {},
   "source": [
    "###  Creating augmented data from random transformations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "609750ac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-03T11:16:13.768087Z",
     "start_time": "2022-01-03T11:16:10.934772Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not returning the initial given images\n",
      "------------------------------------------------------\n",
      "Total amount of images after transformation  0  = 1100\n",
      "Not returning the initial given images\n",
      "------------------------------------------------------\n",
      "Total amount of images after transformation  1  = 2100\n",
      "Not returning the initial given images\n",
      "------------------------------------------------------\n",
      "Total amount of images after transformation  2  = 3100\n",
      "Not returning the initial given images\n",
      "------------------------------------------------------\n",
      "Total amount of images after transformation  3  = 4100\n",
      "Not returning the initial given images\n",
      "------------------------------------------------------\n",
      "Total amount of images after transformation  4  = 5100\n",
      "Not returning the initial given images\n",
      "------------------------------------------------------\n",
      "Total amount of images after transformation  5  = 6100\n",
      "Not returning the initial given images\n",
      "------------------------------------------------------\n",
      "Total amount of images after transformation  6  = 7100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torchvision\\transforms\\functional_tensor.py:876: UserWarning: Argument fill/fillcolor is not supported for Tensor input. Fill value is zero\n",
      "  warnings.warn(\"Argument fill/fillcolor is not supported for Tensor input. Fill value is zero\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not returning the initial given images\n",
      "------------------------------------------------------\n",
      "Total amount of images after transformation  7  = 8100\n",
      "\n",
      "Total amount of images =  8100\n"
     ]
    }
   ],
   "source": [
    "### for loop to create the augmented data\n",
    "################### input for us to change: ###################\n",
    "imgaes_to_augment = X_train_t[0:100]\n",
    "labels_to_augment = y_train_t[0:100]\n",
    "\n",
    "transformations_list = [tr_ColorJitter_1, tr_ColorJitter_2, tr_ColorJitter_3, tr_ColorJitter_4, tr_ColorJitter_5,\n",
    "                       tr_RandomAffine_1,\n",
    "                       tr_RandomResizedCrop_1,\n",
    "                       tr_flip_blur_1]\n",
    "\n",
    "num_of_replicates = 10\n",
    "#########################################\n",
    "augmented_data = imgaes_to_augment\n",
    "augmented_labels = labels_to_augment\n",
    "######################################### The for loop: ###################\n",
    "\n",
    "for i, transform_1 in enumerate(transformations_list):\n",
    "    new_images, new_labels = create_augemnted_data(images = imgaes_to_augment, labels = labels_to_augment,\n",
    "                                                   tranform = transform_1,\n",
    "                                                   num_of_replicates = num_of_replicates,\n",
    "                                                   random_state = 1998,\n",
    "                                                   verbose = False,\n",
    "                                                   return_the_initial_images = False)\n",
    "\n",
    "    augmented_data =  torch.cat((augmented_data, new_images), dim = 0) # add the new transformed images\n",
    "    augmented_labels = torch.cat((augmented_labels, new_labels), dim = 0) #add the labels of the new transformed images\n",
    "    print(\"Total amount of images after transformation \", i, \" =\", augmented_data.shape[0])\n",
    "    \n",
    "print()\n",
    "print(\"Total amount of images = \", augmented_data.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274ae00a",
   "metadata": {},
   "source": [
    "# Creating augemnted data and modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c7e801ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T13:48:23.685295Z",
     "start_time": "2022-01-12T13:46:39.814522Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input data shape: torch.Size([47500, 3, 32, 32])\n",
      "\n",
      "Not returning the initial given images\n",
      "------------------------------------------------------\n",
      "Total amount of images after transformation  0  = 190000\n",
      "\n",
      "Total amount of images =  190000\n"
     ]
    }
   ],
   "source": [
    "### for loop to create the augmented data\n",
    "################### input for us to change: ###################\n",
    "imgaes_to_augment = X_train_t_normalize #[0:100]\n",
    "labels_to_augment = y_train_t #[0:100]\n",
    "print(\"input data shape:\", imgaes_to_augment.shape)\n",
    "print()\n",
    "\n",
    "transformations_list = [tr]\n",
    "\n",
    "#transformations_list = [tr_ColorJitter_1, tr_ColorJitter_2,tr_flip_blur_1]\n",
    "\n",
    "#transformations_list = [tr_ColorJitter_1, tr_ColorJitter_2,  tr_ColorJitter_5,\n",
    "#                       tr_RandomAffine_1, tr_RandomResizedCrop_1, tr_flip_blur_1]\n",
    "\n",
    "#transformations_list = [tr_ColorJitter_1, tr_ColorJitter_2, tr_ColorJitter_3, tr_ColorJitter_4, tr_ColorJitter_5,\n",
    "#                       tr_RandomAffine_1,\n",
    "#                       tr_RandomResizedCrop_1,\n",
    "#                       tr_flip_blur_1]\n",
    "\n",
    "num_of_replicates = 3\n",
    "#########################################\n",
    "augmented_data = imgaes_to_augment\n",
    "augmented_labels = labels_to_augment\n",
    "######################################### The for loop: ###################\n",
    "\n",
    "for i, transform_1 in enumerate(transformations_list):\n",
    "    new_images, new_labels = create_augemnted_data(images = imgaes_to_augment, labels = labels_to_augment,\n",
    "                                                   tranform = transform_1,\n",
    "                                                   num_of_replicates = num_of_replicates,\n",
    "                                                   random_state = 1998,\n",
    "                                                   verbose = False,\n",
    "                                                   return_the_initial_images = False)\n",
    "\n",
    "    augmented_data =  torch.cat((augmented_data, new_images), dim = 0) # add the new transformed images\n",
    "    augmented_labels = torch.cat((augmented_labels, new_labels), dim = 0) #add the labels of the new transformed images\n",
    "    print(\"Total amount of images after transformation \", i, \" =\", augmented_data.shape[0])\n",
    "    \n",
    "print()\n",
    "print(\"Total amount of images = \", augmented_data.shape[0])\n",
    "\n",
    "X_train_t_normalize_augmented = augmented_data\n",
    "y_train_t_augmented = augmented_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f41d16",
   "metadata": {},
   "source": [
    "## Modeling with the augmented data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db40679a",
   "metadata": {},
   "source": [
    "## Model 1 - without convolution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b6b4b899",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-08T16:41:51.067756Z",
     "start_time": "2022-01-08T15:28:36.317530Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py:117: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 of 40 | Loss: 1.9597 | Train accuracy: 0.3800 | Validate accuracy: 0.4204\n",
      "1 of 40 | Loss: 1.8010 | Train accuracy: 0.3680 | Validate accuracy: 0.4692\n",
      "2 of 40 | Loss: 1.7364 | Train accuracy: 0.4300 | Validate accuracy: 0.4504\n",
      "3 of 40 | Loss: 1.6907 | Train accuracy: 0.4320 | Validate accuracy: 0.4572\n",
      "4 of 40 | Loss: 1.6579 | Train accuracy: 0.4600 | Validate accuracy: 0.4996\n",
      "5 of 40 | Loss: 1.6292 | Train accuracy: 0.4440 | Validate accuracy: 0.5016\n",
      "6 of 40 | Loss: 1.6048 | Train accuracy: 0.4540 | Validate accuracy: 0.5000\n",
      "7 of 40 | Loss: 1.5815 | Train accuracy: 0.4360 | Validate accuracy: 0.5052\n",
      "8 of 40 | Loss: 1.5624 | Train accuracy: 0.4620 | Validate accuracy: 0.5188\n",
      "9 of 40 | Loss: 1.5478 | Train accuracy: 0.4680 | Validate accuracy: 0.5208\n",
      "10 of 40 | Loss: 1.5303 | Train accuracy: 0.5380 | Validate accuracy: 0.5220\n",
      "11 of 40 | Loss: 1.5178 | Train accuracy: 0.5280 | Validate accuracy: 0.5092\n",
      "12 of 40 | Loss: 1.5035 | Train accuracy: 0.4900 | Validate accuracy: 0.5344\n",
      "13 of 40 | Loss: 1.4922 | Train accuracy: 0.5240 | Validate accuracy: 0.5072\n",
      "14 of 40 | Loss: 1.4783 | Train accuracy: 0.5080 | Validate accuracy: 0.5316\n",
      "15 of 40 | Loss: 1.4679 | Train accuracy: 0.5460 | Validate accuracy: 0.5404\n",
      "16 of 40 | Loss: 1.4580 | Train accuracy: 0.5760 | Validate accuracy: 0.5240\n",
      "17 of 40 | Loss: 1.4461 | Train accuracy: 0.5760 | Validate accuracy: 0.5488\n",
      "18 of 40 | Loss: 1.4369 | Train accuracy: 0.5640 | Validate accuracy: 0.5320\n",
      "19 of 40 | Loss: 1.4280 | Train accuracy: 0.6040 | Validate accuracy: 0.5572\n",
      "20 of 40 | Loss: 1.4182 | Train accuracy: 0.5820 | Validate accuracy: 0.5460\n",
      "21 of 40 | Loss: 1.4112 | Train accuracy: 0.5740 | Validate accuracy: 0.5456\n",
      "22 of 40 | Loss: 1.4040 | Train accuracy: 0.5540 | Validate accuracy: 0.5448\n",
      "23 of 40 | Loss: 1.3947 | Train accuracy: 0.5440 | Validate accuracy: 0.5444\n",
      "24 of 40 | Loss: 1.3873 | Train accuracy: 0.5820 | Validate accuracy: 0.5484\n",
      "25 of 40 | Loss: 1.3822 | Train accuracy: 0.6020 | Validate accuracy: 0.5484\n",
      "26 of 40 | Loss: 1.3755 | Train accuracy: 0.5900 | Validate accuracy: 0.5504\n",
      "27 of 40 | Loss: 1.3722 | Train accuracy: 0.5760 | Validate accuracy: 0.5556\n",
      "28 of 40 | Loss: 1.3656 | Train accuracy: 0.6020 | Validate accuracy: 0.5492\n",
      "29 of 40 | Loss: 1.3591 | Train accuracy: 0.6080 | Validate accuracy: 0.5540\n",
      "30 of 40 | Loss: 1.3521 | Train accuracy: 0.5900 | Validate accuracy: 0.5360\n",
      "31 of 40 | Loss: 1.3444 | Train accuracy: 0.5880 | Validate accuracy: 0.5536\n",
      "32 of 40 | Loss: 1.3396 | Train accuracy: 0.6020 | Validate accuracy: 0.5476\n",
      "33 of 40 | Loss: 1.3382 | Train accuracy: 0.5940 | Validate accuracy: 0.5500\n",
      "34 of 40 | Loss: 1.3326 | Train accuracy: 0.6460 | Validate accuracy: 0.5396\n",
      "35 of 40 | Loss: 1.3316 | Train accuracy: 0.6200 | Validate accuracy: 0.5412\n",
      "36 of 40 | Loss: 1.3248 | Train accuracy: 0.6120 | Validate accuracy: 0.5312\n",
      "37 of 40 | Loss: 1.3217 | Train accuracy: 0.6000 | Validate accuracy: 0.5472\n",
      "38 of 40 | Loss: 1.3149 | Train accuracy: 0.6380 | Validate accuracy: 0.5364\n",
      "39 of 40 | Loss: 1.3093 | Train accuracy: 0.6340 | Validate accuracy: 0.5392\n",
      "\n",
      "Total train time =  0.0645750740118194  seconds\n",
      "Total train time =  0.001  minutes\n"
     ]
    }
   ],
   "source": [
    "#all together:\n",
    "model_augmented = Net_models(layers=[650, 512, 256, 150], output_layer = [10],\n",
    "                             drop_out=0.2,\n",
    "                             batch_norm = True).to(device=device)\n",
    "criterion = torch.nn.CrossEntropyLoss() #The loss function\n",
    "optimizer = torch.optim.SGD(model_augmented.parameters(),lr = 0.4,  momentum=0.7) #The optimizer\n",
    "\n",
    "n_epochs = 50\n",
    "batch_size = 500\n",
    "\n",
    "model_augmented_new, progress_log = train_MLP_net_images(model = model_augmented,\n",
    "                                 x_train_normalize_t = X_train_t_normalize_augmented , y_train_t = y_train_t_augmented,\n",
    "                                 x_validation_normalize_t = X_validation_t_normalize, y_validation_t = y_validation_t,\n",
    "                                 criterion = criterion, optimizer = optimizer,\n",
    "                                 n_epochs = n_epochs, batch_size = batch_size, device = device)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e765a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e7f3606a",
   "metadata": {},
   "source": [
    "## Model 2  - with convolution and without maxpool:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b8784326",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-08T17:35:39.080991Z",
     "start_time": "2022-01-08T16:41:51.763732Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 of 60 | Loss: 2.0819 | Train accuracy: 0.3000 | Validate accuracy: 0.3316\n",
      "1 of 60 | Loss: 1.8932 | Train accuracy: 0.2920 | Validate accuracy: 0.3808\n",
      "2 of 60 | Loss: 1.7832 | Train accuracy: 0.3960 | Validate accuracy: 0.4600\n",
      "3 of 60 | Loss: 1.6933 | Train accuracy: 0.4020 | Validate accuracy: 0.4764\n",
      "4 of 60 | Loss: 1.6269 | Train accuracy: 0.4520 | Validate accuracy: 0.4972\n",
      "5 of 60 | Loss: 1.5795 | Train accuracy: 0.4660 | Validate accuracy: 0.4992\n",
      "6 of 60 | Loss: 1.5251 | Train accuracy: 0.4740 | Validate accuracy: 0.5084\n",
      "7 of 60 | Loss: 1.4765 | Train accuracy: 0.5480 | Validate accuracy: 0.5244\n",
      "8 of 60 | Loss: 1.4298 | Train accuracy: 0.5440 | Validate accuracy: 0.5132\n",
      "9 of 60 | Loss: 1.4053 | Train accuracy: 0.5440 | Validate accuracy: 0.5188\n",
      "10 of 60 | Loss: 1.3554 | Train accuracy: 0.5880 | Validate accuracy: 0.5092\n",
      "11 of 60 | Loss: 1.3174 | Train accuracy: 0.5820 | Validate accuracy: 0.5216\n",
      "12 of 60 | Loss: 1.3160 | Train accuracy: 0.5940 | Validate accuracy: 0.5140\n",
      "13 of 60 | Loss: 1.2522 | Train accuracy: 0.6040 | Validate accuracy: 0.5340\n",
      "14 of 60 | Loss: 1.2223 | Train accuracy: 0.6320 | Validate accuracy: 0.5352\n",
      "15 of 60 | Loss: 1.1891 | Train accuracy: 0.6420 | Validate accuracy: 0.5332\n",
      "16 of 60 | Loss: 1.1630 | Train accuracy: 0.6540 | Validate accuracy: 0.5356\n",
      "17 of 60 | Loss: 1.1350 | Train accuracy: 0.6460 | Validate accuracy: 0.5224\n",
      "18 of 60 | Loss: 1.1129 | Train accuracy: 0.6980 | Validate accuracy: 0.5212\n",
      "19 of 60 | Loss: 1.0941 | Train accuracy: 0.6780 | Validate accuracy: 0.5180\n",
      "20 of 60 | Loss: 1.0610 | Train accuracy: 0.6920 | Validate accuracy: 0.5224\n",
      "21 of 60 | Loss: 1.0397 | Train accuracy: 0.6800 | Validate accuracy: 0.5204\n",
      "22 of 60 | Loss: 1.0205 | Train accuracy: 0.7100 | Validate accuracy: 0.5144\n",
      "23 of 60 | Loss: 1.0101 | Train accuracy: 0.7720 | Validate accuracy: 0.5140\n",
      "24 of 60 | Loss: 0.9829 | Train accuracy: 0.7200 | Validate accuracy: 0.5212\n",
      "25 of 60 | Loss: 0.9659 | Train accuracy: 0.7080 | Validate accuracy: 0.5100\n",
      "26 of 60 | Loss: 0.9498 | Train accuracy: 0.7720 | Validate accuracy: 0.5076\n",
      "27 of 60 | Loss: 0.9299 | Train accuracy: 0.7840 | Validate accuracy: 0.4996\n",
      "28 of 60 | Loss: 0.9167 | Train accuracy: 0.7600 | Validate accuracy: 0.5088\n",
      "29 of 60 | Loss: 0.9012 | Train accuracy: 0.8020 | Validate accuracy: 0.5024\n",
      "30 of 60 | Loss: 0.8875 | Train accuracy: 0.7660 | Validate accuracy: 0.5044\n",
      "31 of 60 | Loss: 0.8731 | Train accuracy: 0.7920 | Validate accuracy: 0.4988\n",
      "32 of 60 | Loss: 0.8589 | Train accuracy: 0.7840 | Validate accuracy: 0.5040\n",
      "33 of 60 | Loss: 0.8477 | Train accuracy: 0.7500 | Validate accuracy: 0.5040\n",
      "34 of 60 | Loss: 0.8360 | Train accuracy: 0.7980 | Validate accuracy: 0.5044\n",
      "35 of 60 | Loss: 0.8254 | Train accuracy: 0.7960 | Validate accuracy: 0.4932\n",
      "36 of 60 | Loss: 0.8144 | Train accuracy: 0.7940 | Validate accuracy: 0.5088\n",
      "37 of 60 | Loss: 0.8012 | Train accuracy: 0.8280 | Validate accuracy: 0.5160\n",
      "38 of 60 | Loss: 0.7941 | Train accuracy: 0.8300 | Validate accuracy: 0.5004\n",
      "39 of 60 | Loss: 0.7847 | Train accuracy: 0.8020 | Validate accuracy: 0.4896\n",
      "40 of 60 | Loss: 0.7740 | Train accuracy: 0.7840 | Validate accuracy: 0.4916\n",
      "41 of 60 | Loss: 0.7672 | Train accuracy: 0.8040 | Validate accuracy: 0.4976\n",
      "42 of 60 | Loss: 0.7573 | Train accuracy: 0.8400 | Validate accuracy: 0.4936\n",
      "43 of 60 | Loss: 0.7492 | Train accuracy: 0.8700 | Validate accuracy: 0.4972\n",
      "44 of 60 | Loss: 0.7388 | Train accuracy: 0.8260 | Validate accuracy: 0.4992\n",
      "45 of 60 | Loss: 0.7293 | Train accuracy: 0.8260 | Validate accuracy: 0.5016\n",
      "46 of 60 | Loss: 0.7250 | Train accuracy: 0.8300 | Validate accuracy: 0.5044\n",
      "47 of 60 | Loss: 0.7199 | Train accuracy: 0.8220 | Validate accuracy: 0.5064\n",
      "48 of 60 | Loss: 0.7120 | Train accuracy: 0.8260 | Validate accuracy: 0.4864\n",
      "49 of 60 | Loss: 0.7039 | Train accuracy: 0.8160 | Validate accuracy: 0.4932\n",
      "50 of 60 | Loss: 0.6986 | Train accuracy: 0.8500 | Validate accuracy: 0.4880\n",
      "51 of 60 | Loss: 0.6897 | Train accuracy: 0.8640 | Validate accuracy: 0.4848\n",
      "52 of 60 | Loss: 0.6844 | Train accuracy: 0.8380 | Validate accuracy: 0.5008\n",
      "53 of 60 | Loss: 0.6773 | Train accuracy: 0.8440 | Validate accuracy: 0.4932\n",
      "54 of 60 | Loss: 0.6727 | Train accuracy: 0.8680 | Validate accuracy: 0.5044\n",
      "55 of 60 | Loss: 0.6672 | Train accuracy: 0.8540 | Validate accuracy: 0.4976\n",
      "56 of 60 | Loss: 0.6653 | Train accuracy: 0.8740 | Validate accuracy: 0.4956\n",
      "57 of 60 | Loss: 0.6567 | Train accuracy: 0.8600 | Validate accuracy: 0.5044\n",
      "58 of 60 | Loss: 0.6489 | Train accuracy: 0.8700 | Validate accuracy: 0.4932\n",
      "59 of 60 | Loss: 0.6482 | Train accuracy: 0.8600 | Validate accuracy: 0.4960\n",
      "\n",
      "Total train time =  0.11374901805007666  seconds\n",
      "Total train time =  0.002  minutes\n"
     ]
    }
   ],
   "source": [
    "#all together:\n",
    "model_conv_augmented = Net_models_conv(C_in = 3, H_in = 32, W_in = 32,\n",
    "                             layers=[512, 256, 150, 100], output_layer = [10],\n",
    "                             drop_out=0.1,\n",
    "                             batch_norm = True).to(device=device)\n",
    "criterion = torch.nn.CrossEntropyLoss() #The loss function\n",
    "optimizer = torch.optim.SGD(model_conv_augmented.parameters(),lr = 0.47, momentum=0.7) #The optimizer\n",
    "\n",
    "n_epochs = 50\n",
    "batch_size = 500\n",
    "\n",
    "model_conv_augmented_new, progress_log = train_MLP_net_images(model = model_conv_augmented,\n",
    "                                 x_train_normalize_t = X_train_t_normalize_augmented , y_train_t = y_train_t_augmented,\n",
    "                                 x_validation_normalize_t = X_validation_t_normalize, y_validation_t = y_validation_t,\n",
    "                                 criterion = criterion, optimizer = optimizer,\n",
    "                                 n_epochs = n_epochs, batch_size = batch_size, device = device)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08520c68",
   "metadata": {},
   "source": [
    "## Model 3  - with convolution and  maxpool:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e9bed81c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 of 50 | Loss: 1.9700 | Train accuracy: 0.3367 | Validate accuracy: 0.3680\n",
      "1 of 50 | Loss: 1.7692 | Train accuracy: 0.3917 | Validate accuracy: 0.4096\n",
      "2 of 50 | Loss: 1.6796 | Train accuracy: 0.3900 | Validate accuracy: 0.4636\n",
      "3 of 50 | Loss: 1.6269 | Train accuracy: 0.3850 | Validate accuracy: 0.4576\n",
      "4 of 50 | Loss: 1.5876 | Train accuracy: 0.3833 | Validate accuracy: 0.4356\n",
      "5 of 50 | Loss: 1.5597 | Train accuracy: 0.4517 | Validate accuracy: 0.5224\n",
      "6 of 50 | Loss: 1.5370 | Train accuracy: 0.4267 | Validate accuracy: 0.4784\n",
      "7 of 50 | Loss: 1.5141 | Train accuracy: 0.4650 | Validate accuracy: 0.5420\n",
      "8 of 50 | Loss: 1.4987 | Train accuracy: 0.4300 | Validate accuracy: 0.5144\n",
      "9 of 50 | Loss: 1.4842 | Train accuracy: 0.4517 | Validate accuracy: 0.5156\n",
      "10 of 50 | Loss: 1.4727 | Train accuracy: 0.3633 | Validate accuracy: 0.4564\n",
      "11 of 50 | Loss: 1.4616 | Train accuracy: 0.4367 | Validate accuracy: 0.5116\n",
      "12 of 50 | Loss: 1.4526 | Train accuracy: 0.4850 | Validate accuracy: 0.5276\n",
      "13 of 50 | Loss: 1.4422 | Train accuracy: 0.4267 | Validate accuracy: 0.5124\n",
      "14 of 50 | Loss: 1.4356 | Train accuracy: 0.4617 | Validate accuracy: 0.5368\n",
      "15 of 50 | Loss: 1.4284 | Train accuracy: 0.3933 | Validate accuracy: 0.4816\n",
      "16 of 50 | Loss: 1.4215 | Train accuracy: 0.4183 | Validate accuracy: 0.5268\n",
      "17 of 50 | Loss: 1.4126 | Train accuracy: 0.4333 | Validate accuracy: 0.5204\n",
      "18 of 50 | Loss: 1.4078 | Train accuracy: 0.4917 | Validate accuracy: 0.5476\n",
      "19 of 50 | Loss: 1.4015 | Train accuracy: 0.3767 | Validate accuracy: 0.4660\n",
      "20 of 50 | Loss: 1.3964 | Train accuracy: 0.4600 | Validate accuracy: 0.4948\n",
      "21 of 50 | Loss: 1.3933 | Train accuracy: 0.4450 | Validate accuracy: 0.4912\n",
      "22 of 50 | Loss: 1.3878 | Train accuracy: 0.5483 | Validate accuracy: 0.5712\n",
      "23 of 50 | Loss: 1.3837 | Train accuracy: 0.4883 | Validate accuracy: 0.5292\n",
      "24 of 50 | Loss: 1.3788 | Train accuracy: 0.4683 | Validate accuracy: 0.5428\n",
      "25 of 50 | Loss: 1.3748 | Train accuracy: 0.4717 | Validate accuracy: 0.5580\n",
      "26 of 50 | Loss: 1.3724 | Train accuracy: 0.4367 | Validate accuracy: 0.5080\n",
      "27 of 50 | Loss: 1.3683 | Train accuracy: 0.5317 | Validate accuracy: 0.5824\n",
      "28 of 50 | Loss: 1.3640 | Train accuracy: 0.5117 | Validate accuracy: 0.5772\n",
      "29 of 50 | Loss: 1.3602 | Train accuracy: 0.5117 | Validate accuracy: 0.5824\n",
      "30 of 50 | Loss: 1.3590 | Train accuracy: 0.4467 | Validate accuracy: 0.5220\n",
      "31 of 50 | Loss: 1.3570 | Train accuracy: 0.5017 | Validate accuracy: 0.5592\n",
      "32 of 50 | Loss: 1.3537 | Train accuracy: 0.5183 | Validate accuracy: 0.5884\n",
      "33 of 50 | Loss: 1.3479 | Train accuracy: 0.5200 | Validate accuracy: 0.5768\n",
      "34 of 50 | Loss: 1.3465 | Train accuracy: 0.5167 | Validate accuracy: 0.5720\n",
      "35 of 50 | Loss: 1.3435 | Train accuracy: 0.5350 | Validate accuracy: 0.5864\n",
      "36 of 50 | Loss: 1.3403 | Train accuracy: 0.5133 | Validate accuracy: 0.5636\n",
      "37 of 50 | Loss: 1.3392 | Train accuracy: 0.5067 | Validate accuracy: 0.5596\n",
      "38 of 50 | Loss: 1.3353 | Train accuracy: 0.5400 | Validate accuracy: 0.5612\n",
      "39 of 50 | Loss: 1.3340 | Train accuracy: 0.4883 | Validate accuracy: 0.5284\n",
      "40 of 50 | Loss: 1.3323 | Train accuracy: 0.4767 | Validate accuracy: 0.5784\n",
      "41 of 50 | Loss: 1.3286 | Train accuracy: 0.5050 | Validate accuracy: 0.5744\n",
      "42 of 50 | Loss: 1.3283 | Train accuracy: 0.5083 | Validate accuracy: 0.5476\n",
      "43 of 50 | Loss: 1.3272 | Train accuracy: 0.5083 | Validate accuracy: 0.5724\n",
      "44 of 50 | Loss: 1.3247 | Train accuracy: 0.5117 | Validate accuracy: 0.5728\n",
      "45 of 50 | Loss: 1.3221 | Train accuracy: 0.5417 | Validate accuracy: 0.5804\n",
      "46 of 50 | Loss: 1.3195 | Train accuracy: 0.5100 | Validate accuracy: 0.5532\n",
      "47 of 50 | Loss: 1.3191 | Train accuracy: 0.5133 | Validate accuracy: 0.5592\n",
      "48 of 50 | Loss: 1.3168 | Train accuracy: 0.5333 | Validate accuracy: 0.5784\n",
      "49 of 50 | Loss: 1.3159 | Train accuracy: 0.5467 | Validate accuracy: 0.5684\n",
      "\n",
      "Total train time =  0.11770384939093338  seconds\n",
      "Total train time =  0.002  minutes\n",
      "total time =  12 3 min\n"
     ]
    }
   ],
   "source": [
    "#all together:\n",
    "start_time = time.time()\n",
    "model_conv_pool_augmented = Net_models_conv_maxpool(C_in = 3, H_in = 32, W_in = 32,\n",
    "                layers=[600, 450, 300, 100], output_layer = [10],\n",
    "                 drop_out = 0.20,\n",
    "                 batch_norm = True,\n",
    "                conv1_out_channels = 35, conv1_kernel_size = 4, conv1_padding = 3, maxpool1_kernel_size = 2,\n",
    "                conv2_out_channels = 20, conv2_kernel_size = 5, conv2_padding = 2, maxpool2_kernel_size = 4,\n",
    "                conv3_out_channels = 10, conv3_kernel_size = 5, conv3_padding = 2, maxpool3_kernel_size = 4).to(device=device)\n",
    "                \n",
    "criterion = torch.nn.CrossEntropyLoss() #The loss function\n",
    "optimizer = torch.optim.SGD(model_conv_pool_augmented.parameters(),lr = 0.55, momentum=0.7) #The optimizer\n",
    "\n",
    "n_epochs = 50\n",
    "batch_size = 600\n",
    "\n",
    "model_conv_pool_augmented_new, progress_log = train_MLP_net_images(model = model_conv_pool_augmented,\n",
    "                                 x_train_normalize_t = X_train_t_normalize_augmented , y_train_t = y_train_t_augmented,\n",
    "                                 x_validation_normalize_t = X_validation_t_normalize, y_validation_t = y_validation_t,\n",
    "                                 criterion = criterion, optimizer = optimizer,\n",
    "                                 n_epochs = n_epochs, batch_size = batch_size, device = device)\n",
    "\n",
    "print(\"total time = \", round((time.time()-start_time)/60),3, \"min\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9ca43d",
   "metadata": {},
   "source": [
    "# Summarizing the models performance on train, validation and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7f0b2a68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating model  1\n",
      "evaluating model  2\n",
      "evaluating model  3\n"
     ]
    }
   ],
   "source": [
    "models = [model_augmented, model_conv_augmented, model_conv_pool_augmented]\n",
    "\n",
    "train_accuracy_list = []\n",
    "validate_accuracy_list = []\n",
    "test_accuracy_list = []\n",
    "testing_time_list = []\n",
    "\n",
    "train_images_norm = X_train_t_normalize.to(device=\"cpu\")\n",
    "train_labels = y_train_t.to(device=\"cpu\")\n",
    "validation_images = X_validation_t_normalize.to(device=\"cpu\")\n",
    "validation_labels = y_validation_t.to(device=\"cpu\")\n",
    "i=0\n",
    "for model0 in models:\n",
    "    i+=1\n",
    "    print(\"evaluating model \", i)\n",
    "    model0.to(device=\"cpu\")\n",
    "    train_accuracy = model_accuracy(model0, train_images_norm, train_labels)\n",
    "    validate_accuracy = model_accuracy(model0, validation_images, validation_labels)  \n",
    "\n",
    "    start_testing_time = time.time() #measure the testing time\n",
    "    test_accuracy = model_accuracy(model0, X_test_t_normalize, y_test_t)  \n",
    "    testing_time_i = time.time() - start_testing_time #measure the testing time\n",
    "\n",
    "    train_accuracy_list.append(train_accuracy)\n",
    "    validate_accuracy_list.append(validate_accuracy)\n",
    "    test_accuracy_list.append(test_accuracy)\n",
    "    testing_time_list.append(testing_time_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "395a8861",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>number of paramaters</th>\n",
       "      <th>Train Accuracy</th>\n",
       "      <th>Validation Accuracy</th>\n",
       "      <th>Test Accuracy</th>\n",
       "      <th>Test time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>model 1</th>\n",
       "      <td>2505286</td>\n",
       "      <td>0.713305</td>\n",
       "      <td>0.5392</td>\n",
       "      <td>0.5428</td>\n",
       "      <td>0.317125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model 2</th>\n",
       "      <td>2159648</td>\n",
       "      <td>0.979537</td>\n",
       "      <td>0.4960</td>\n",
       "      <td>0.4895</td>\n",
       "      <td>3.052330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model 3</th>\n",
       "      <td>470735</td>\n",
       "      <td>0.649811</td>\n",
       "      <td>0.5684</td>\n",
       "      <td>0.5657</td>\n",
       "      <td>6.480661</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         number of paramaters  Train Accuracy  Validation Accuracy  \\\n",
       "model 1               2505286        0.713305               0.5392   \n",
       "model 2               2159648        0.979537               0.4960   \n",
       "model 3                470735        0.649811               0.5684   \n",
       "\n",
       "         Test Accuracy  Test time  \n",
       "model 1         0.5428   0.317125  \n",
       "model 2         0.4895   3.052330  \n",
       "model 3         0.5657   6.480661  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_params_list = []\n",
    "model_names_list = []\n",
    "for i in range(len(models)):\n",
    "    total_params_list.append(sum(p.numel() for p in models[i].parameters()))\n",
    "    model_name_i = \"model \" + str(i+1)\n",
    "    model_names_list.append(model_name_i)\n",
    "\n",
    "df_summary_all_augmented = pd.DataFrame({\"number of paramaters\" : total_params_list,\n",
    "                               \"Train Accuracy\" : np.array(train_accuracy_list),\n",
    "                               \"Validation Accuracy\" : np.array(validate_accuracy_list),\n",
    "                               \"Test Accuracy\" : np.array(test_accuracy_list),\n",
    "                               \"Test time\" : testing_time_list\n",
    "                               },\n",
    "                              index = [model_names_list])\n",
    "df_summary_all_augmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007827fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### lets compare to the model's performance with the original data (not augmented)\n",
    "df_summary_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4848fe78",
   "metadata": {},
   "source": [
    "### Conclusions PART  B - We can see that the models that are trained with the *augmented data are better performing* than the same models that were trained on the regular data. This make sense as the models had more training points. We know that increasing the training data (as with augmentation) decreases a model's variance and ultimately decreases regularization error. Moreover, the benefit of data augmentation generally comes down to reducing overfitting. For example, a classification model trained on three images will be limited to recognizing and classifying those exact images. Even adding slight variations to data will improve the generalizability, as we saw in real life with CIFAR 10 data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caef58de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872a18ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af08dd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d304ce07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55ba7120",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-06T09:26:33.105209Z",
     "start_time": "2022-01-06T09:26:33.101290Z"
    }
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "del optimizer\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "795.844px",
    "left": "1524px",
    "right": "20px",
    "top": "123px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
